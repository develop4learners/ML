{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLED Final Project",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/develop4learners/RatioRancher-Game-ML-Project/blob/master/MLED_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YR5PS3pMe0C1",
        "colab_type": "code",
        "outputId": "3cf10b41-049d-40a3-b31f-6648856ec22c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sethcorrigan/prima_process\n",
        "%cd prima_process"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'prima_process'...\n",
            "remote: Enumerating objects: 9, done.\u001b[K\n",
            "remote: Total 9 (delta 0), reused 0 (delta 0), pack-reused 9\u001b[K\n",
            "Unpacking objects: 100% (9/9), done.\n",
            "/content/prima_process/prepost/prima_process\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "VnesXPOS1UCN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "all_primadata_df = pd.read_csv('all_primadata.csv', sep=\",\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oGH9aZBNdq23",
        "colab_type": "code",
        "outputId": "2eb816a2-fd2d-42e9-994b-003cc3138b11",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        }
      },
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aybolek/prepost\n",
        "%cd prepost"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'prepost'...\n",
            "remote: Enumerating objects: 6, done.\u001b[K\n",
            "remote: Counting objects:  16% (1/6)   \u001b[K\rremote: Counting objects:  33% (2/6)   \u001b[K\rremote: Counting objects:  50% (3/6)   \u001b[K\rremote: Counting objects:  66% (4/6)   \u001b[K\rremote: Counting objects:  83% (5/6)   \u001b[K\rremote: Counting objects: 100% (6/6)   \u001b[K\rremote: Counting objects: 100% (6/6), done.\u001b[K\n",
            "remote: Compressing objects:  20% (1/5)   \u001b[K\rremote: Compressing objects:  40% (2/5)   \u001b[K\rremote: Compressing objects:  60% (3/5)   \u001b[K\rremote: Compressing objects:  80% (4/5)   \u001b[K\rremote: Compressing objects: 100% (5/5)   \u001b[K\rremote: Compressing objects: 100% (5/5), done.\u001b[K\n",
            "remote: Total 6 (delta 0), reused 0 (delta 0), pack-reused 0\u001b[K\n",
            "Unpacking objects:  16% (1/6)   \rUnpacking objects:  33% (2/6)   \rUnpacking objects:  50% (3/6)   \rUnpacking objects:  66% (4/6)   \rUnpacking objects:  83% (5/6)   \rUnpacking objects: 100% (6/6)   \rUnpacking objects: 100% (6/6), done.\n",
            "/content/prima_process/prepost/prima_process/prepost\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "S6mb5MKvduc6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "pre_df = pd.read_csv('pre.csv')\n",
        "post_df = pd.read_csv('post.csv')\n",
        "pre_test = pd.DataFrame()\n",
        "pre_test['userId'] = pre_df['uid']\n",
        "pre_test['pre_raw'] = pre_df['pr_raw']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WmAIWVDGd_u4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "post_test = pd.DataFrame()\n",
        "post_test['userId'] = post_df['uid']\n",
        "post_test['post_raw'] = post_df['post-rawscr']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tgl2zSI4eDMa",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pre_post = pd.merge(pre_test, post_test, on=\"userId\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "n4mTKR8AhaRi",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df = all_primadata_df"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CciLdq5MmQoz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['Unnamed: 10'] = df['Unnamed: 10'].str.replace('previous_pen_dimensions:', '')\n",
        "df['Unnamed: 9'] = df['Unnamed: 9'].str.replace('pen_dimensions:', '')\n",
        "df['Unnamed: 9'] = df['Unnamed: 9'].str.replace('_', ' ')\n",
        "df['Unnamed: 9'] = df['Unnamed: 9'].str.replace('\"', '')\n",
        "df['Unnamed: 9'] = df['Unnamed: 9'].str.split()\n",
        "df['Unnamed: 10'] = df['Unnamed: 10'].str.replace('_', ' ')\n",
        "df['Unnamed: 10'] = df['Unnamed: 10'].str.replace('\"', '')\n",
        "df['Unnamed: 10'] = df['Unnamed: 10'].str.split()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HgER7UfHqJoR",
        "colab_type": "code",
        "outputId": "8b35fe33-edc2-4d42-fe4b-b99496194d0d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "if df['Unnamed: 10'][10][1] > df['Unnamed: 9'][10][1]:\n",
        "  print (\"hi\")\n",
        "elif df['Unnamed: 10'][10][1] < df['Unnamed: 9'][10][1]:\n",
        "  print(\"no\")\n",
        "print(int(df['Unnamed: 10'][10][1]) - int(df['Unnamed: 9'][10][1]))'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nif df[\\'Unnamed: 10\\'][10][1] > df[\\'Unnamed: 9\\'][10][1]:\\n  print (\"hi\")\\nelif df[\\'Unnamed: 10\\'][10][1] < df[\\'Unnamed: 9\\'][10][1]:\\n  print(\"no\")\\nprint(int(df[\\'Unnamed: 10\\'][10][1]) - int(df[\\'Unnamed: 9\\'][10][1]))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 115
        }
      ]
    },
    {
      "metadata": {
        "id": "T6hEeiJqucv8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "df['diff in animal area'] = df['Unnamed: 10']\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vzMqvLCEwjpx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "new = df[df[\"actionName\"] == 'resize_pen']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lhd0c72FraCY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new = new.reset_index()\n",
        "#new[\"diff in animal area\"] = new[\"Unnamed: 10\"].apply(lambda x: int(x[0])) - new[\"Unnamed: 9\"].apply(lambda x: int(x[0]))\n",
        "#new[\"diff in vegetable area\"] = new[\"Unnamed: 10\"].apply(lambda x: int(x[1])) - new[\"Unnamed: 9\"].apply(lambda x: int(x[1]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "A6ElBimXICGf",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#new_grouped2 = new.groupby(['gameLevel'])['actionName'].count()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "C6Tj62p2Rz1K",
        "colab_type": "code",
        "outputId": "6aa0d87d-a376-477b-84df-1890e5038d9c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "questions = ['1.05b', '2.01c', '3.02b', '4.07b', 'T1.02a', 'T1.02b', 'T1.04', 'T1.07a', 'T3.01a', 'T4.01a', 'T4.01b', 'T4.02']\n",
        "for index, row in new_grouped.iterrows():\n",
        "    if row['gameLevel'] not in questions:\n",
        "        new_grouped.drop(index, inplace=True)\n",
        "new_grouped'''\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nquestions = ['1.05b', '2.01c', '3.02b', '4.07b', 'T1.02a', 'T1.02b', 'T1.04', 'T1.07a', 'T3.01a', 'T4.01a', 'T4.01b', 'T4.02']\\nfor index, row in new_grouped.iterrows():\\n    if row['gameLevel'] not in questions:\\n        new_grouped.drop(index, inplace=True)\\nnew_grouped\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 119
        }
      ]
    },
    {
      "metadata": {
        "id": "rbmKoMpFbFRS",
        "colab_type": "code",
        "outputId": "0b66343a-6a13-4c90-db34-14a484206321",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "'''for index, row in new_grouped.iterrows():\n",
        "  hardestnew_grouped[new_grouped['gameLevel'] == '1.05b']['count'][row]'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"for index, row in new_grouped.iterrows():\\n  hardestnew_grouped[new_grouped['gameLevel'] == '1.05b']['count'][row]\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 120
        }
      ]
    },
    {
      "metadata": {
        "id": "nYaD-bhtUyUP",
        "colab_type": "code",
        "outputId": "b165398c-dad9-4636-c301-d43044c226f4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "'''new_grouped5 = new_grouped[new_grouped['gameLevel'] == questions[1]]\n",
        "new_grouped5'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"new_grouped5 = new_grouped[new_grouped['gameLevel'] == questions[1]]\\nnew_grouped5\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 121
        }
      ]
    },
    {
      "metadata": {
        "id": "IDPHn-MbTaYp",
        "colab_type": "code",
        "outputId": "beb4010d-7238-424f-b816-12a1b1a1ae7d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "'''new_grouped[new_grouped['gameLevel']== '1.05b', '2.01c']'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"new_grouped[new_grouped['gameLevel']== '1.05b', '2.01c']\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 122
        }
      ]
    },
    {
      "metadata": {
        "id": "KyK5ayhJPTJ_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#hardest_problems [1][1] = new_grouped.where(userIds[1] == new_grouped['userId'] & new_grouped['gameLevel'] == '1.05b')\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tlEh2Rl3P-S3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#new_grouped.bool(userIds[1] == new_grouped['userId'] & new_grouped['gameLevel'] == '1.05b')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bPcLqOWHQKUV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#new_grouped['userId'][]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3o8zM1T5HbI8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_grouped = new.groupby(['userId', 'gameLevel'])['actionName'].count()\n",
        "new_grouped = new_grouped.to_frame()\n",
        "new_grouped.columns = ['count']\n",
        "new_grouped = new_grouped.reset_index()\n",
        "\n",
        "#new_grouped.columns\n",
        "new_mean = new_grouped.groupby(\"userId\")['count'].mean()\n",
        "\n",
        "#I want the average number of attempts per student across all his tasks"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "B3NFwGstiTg_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_means = pd.DataFrame(new_mean, columns = ['userId', 'mean attempts'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RKDMTmlFjDk6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "new_means = new_mean.to_frame()\n",
        "new_means1 = new_means.reset_index()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4eC5gz6IOTpe",
        "colab_type": "code",
        "outputId": "594e2bc2-529a-4b76-fc0c-292fc67e56e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pre_post_mean = pd.merge(pre_post, new_means1, on='userId')\n",
        "pre_post_mean.columns = ['userId', 'pre_raw', 'post_raw', 'mean attempts per student']\n",
        "pre_post_mean['mean attempts per student'].max() - pre_post_mean['mean attempts per student'].min()\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 128
        }
      ]
    },
    {
      "metadata": {
        "id": "B_UHXMMokjAe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pre_post_mean['diff'] = pre_post_mean['post_raw']-pre_post_mean['pre_raw']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FhYJHspDlEvj",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def diffs(x):\n",
        "  if x < 0:\n",
        "    return(\"decrease\")\n",
        "  elif x == 0: \n",
        "    return(\"no_change\")\n",
        "  elif x > 0:\n",
        "    return(\"increase\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "oObdOcFJlg7A",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pre_post_mean[\"differences\"] = pre_post_mean[\"diff\"].apply(lambda x: diffs(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "P7N0Z6Vdn9xV",
        "colab_type": "code",
        "outputId": "0f40bb7f-2b5a-40b2-a01a-362d34c7b65e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pre_post_mean['mean attempts per student'].max() - pre_post_mean['mean attempts per student'].min()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "40.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "metadata": {
        "id": "mVoTBnQroAqI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "low = pre_post_mean['mean attempts per student'].min() + 40/3\n",
        "median = low + 40/3\n",
        "high = median + 40/3"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jEdSA3St2OZK",
        "colab_type": "code",
        "outputId": "87a52da5-1d81-482a-c776-4e7e381c1dee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "print(np.percentile(pre_post_mean['mean attempts per student'], 33))\n",
        "print(np.percentile(pre_post_mean['mean attempts per student'], 67))\n",
        "print(np.percentile(pre_post_mean['mean attempts per student'], 100))'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nprint(np.percentile(pre_post_mean['mean attempts per student'], 33))\\nprint(np.percentile(pre_post_mean['mean attempts per student'], 67))\\nprint(np.percentile(pre_post_mean['mean attempts per student'], 100))\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "metadata": {
        "id": "S5fgdUAhvuEm",
        "colab_type": "code",
        "outputId": "5aee1477-b6fe-4229-fe0a-50702da72844",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pre_post_mean['pre_raw'].max() - pre_post_mean['pre_raw'].min()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 135
        }
      ]
    },
    {
      "metadata": {
        "id": "4Db0VnQSv7yN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "low1 = pre_post_mean['pre_raw'].min() + 8\n",
        "medium1 = low1 + 8\n",
        "high1 = medium1+8"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "zDDA8uauwblt",
        "colab_type": "code",
        "outputId": "86693eeb-fcd4-4c67-bf3f-feb50070c6fc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pre_post_mean['post_raw'].max() - pre_post_mean['post_raw'].min()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 137
        }
      ]
    },
    {
      "metadata": {
        "id": "IgTMklIawmPO",
        "colab_type": "code",
        "outputId": "bd2593e7-c2cf-448d-9558-f8afa4b06a65",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "pre_post_mean['post_raw'].min()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 138
        }
      ]
    },
    {
      "metadata": {
        "id": "almFR5ebpUru",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def groups(x):\n",
        "  if x <= 17.34:\n",
        "    return(\"low\")\n",
        "  elif x <= 30.67 and x > 17.34: \n",
        "    return(\"medium\")\n",
        "  else: # high\n",
        "    return(\"high\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jMGemPAVwrvY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def groups1(x):\n",
        "  if x <= 8:\n",
        "    return(\"low\")\n",
        "  elif x <= 16 and x > 8: \n",
        "    return(\"medium\")\n",
        "  else: # high\n",
        "    return(\"high\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YNq3cyTIoe-c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pre_post_mean[\"groupings\"] = pre_post_mean[\"mean attempts per student\"].apply(lambda x: groups(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "PNSikSQzwzXg",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "pre_post_mean[\"pre_groupings\"] = pre_post_mean[\"pre_raw\"].apply(lambda x: groups1(x))\n",
        "pre_post_mean[\"post_groupings\"] = pre_post_mean[\"post_raw\"].apply(lambda x: groups1(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "OGD4Y2_gq5HP",
        "colab_type": "code",
        "outputId": "de487142-d0b3-47bc-a61a-270fb399eb84",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "pre_post_mean.groupby(\"groupings\")[\"userId\"].count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "groupings\n",
              "high       10\n",
              "low       148\n",
              "medium     87\n",
              "Name: userId, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 143
        }
      ]
    },
    {
      "metadata": {
        "id": "PWSK5pP3w-DG",
        "colab_type": "code",
        "outputId": "5a7ea25b-8693-413f-fc79-a322c4a3350b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "pre_post_mean.groupby(\"pre_groupings\")[\"userId\"].count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "pre_groupings\n",
              "high      121\n",
              "low        40\n",
              "medium     84\n",
              "Name: userId, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 144
        }
      ]
    },
    {
      "metadata": {
        "id": "acF2-menxEPw",
        "colab_type": "code",
        "outputId": "592887d6-e0c4-4605-d05d-168e81ee1165",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "cell_type": "code",
      "source": [
        "pre_post_mean.groupby(\"post_groupings\")[\"userId\"].count()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "post_groupings\n",
              "high      120\n",
              "low        50\n",
              "medium     75\n",
              "Name: userId, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 145
        }
      ]
    },
    {
      "metadata": {
        "id": "ju_xqVfEkg07",
        "colab_type": "code",
        "outputId": "b4724e3f-ca27-469d-f480-7e869e4ff1c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "low2 = pre_post_mean['mean attempts per student'].quantile(.33)\n",
        "medium2 = pre_post_mean['mean attempts per student'].quantile([.34, .67])\n",
        "high2 = pre_post_mean['mean attempts per student'].quantile([.68, 1])'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\nlow2 = pre_post_mean['mean attempts per student'].quantile(.33)\\nmedium2 = pre_post_mean['mean attempts per student'].quantile([.34, .67])\\nhigh2 = pre_post_mean['mean attempts per student'].quantile([.68, 1])\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 146
        }
      ]
    },
    {
      "metadata": {
        "id": "0LA2GqJMRuQz",
        "colab_type": "code",
        "outputId": "81d58035-88aa-4b59-8a37-4962cbdaf7b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "'''gp = new_grouped.groupby(\"userId\")\n",
        "userIds = list(gp.groups.keys())\n",
        "userIds'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gp = new_grouped.groupby(\"userId\")\\nuserIds = list(gp.groups.keys())\\nuserIds'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 147
        }
      ]
    },
    {
      "metadata": {
        "id": "3c-qeOEo9gBv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tFu18W3qGsVX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#type(inputs_array)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TSgTfA25Sd0D",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#pre_post_mean"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wh1hjfxRymUW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preprossessing"
      ]
    },
    {
      "metadata": {
        "id": "Hdtgk0bSFAak",
        "colab_type": "code",
        "outputId": "0f047014-8a5a-44e1-e2dd-596259759005",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "import numpy as np\n",
        "matrix = np.zeros((171,6))\n",
        "matrix\n",
        "\n",
        "matrix_output = np.zeros((171,3))\n",
        "matrix_pre_only = np.zeros((171,3))\n",
        "matrix_actions = np.zeros((171,3))'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nimport numpy as np\\nmatrix = np.zeros((171,6))\\nmatrix\\n\\nmatrix_output = np.zeros((171,3))\\nmatrix_pre_only = np.zeros((171,3))\\nmatrix_actions = np.zeros((171,3))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 151
        }
      ]
    },
    {
      "metadata": {
        "id": "kYOLtByFSTst",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "matrix = np.zeros((245,6))\n",
        "matrix_output = np.zeros((245,3))\n",
        "matrix_pre_only = np.zeros((245,3))\n",
        "matrix_actions = np.zeros((245,3))\n",
        "matrix_diff = np.zeros((245,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sHhjKXu7FGeJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#matrix[1,2] = 9\n",
        "#matrix"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N81SnbpKFpjn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "#for i in range(skill_matrix.shape[0]):\n",
        "    #result[i, np.arange(seq_len), skill_matrix[i]] = 1."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "QQiDY4LUG30n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for i in range(matrix.shape[0]):\n",
        "  #matrix[i, 2] = 4"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "66_wZQNnKK0v",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(matrix_pre_only.shape[0]):\n",
        "  if pre_post_mean[\"pre_groupings\"][i] =='low': #this is wrong, fix it  \n",
        "    matrix_pre_only[i,0] = 1\n",
        "  elif pre_post_mean[\"pre_groupings\"][i] == 'medium':\n",
        "    matrix_pre_only[i,1] = 1\n",
        "  elif pre_post_mean[\"pre_groupings\"][i] == 'high':\n",
        "    matrix_pre_only[i,2] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "COAzpNrqHfPZ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(matrix.shape[0]):\n",
        "  if pre_post_mean[\"pre_groupings\"][i] =='low': #this is wrong, fix it  \n",
        "    matrix[i,0] = 1\n",
        "  elif pre_post_mean[\"pre_groupings\"][i] == 'medium':\n",
        "    matrix[i,1] = 1\n",
        "  elif pre_post_mean[\"pre_groupings\"][i] == 'high':\n",
        "    matrix[i,2] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pgdq4kLkJxbG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(matrix_output.shape[0]):\n",
        "  if pre_post_mean[\"post_groupings\"][i] =='low': #this is wrong, fix it  \n",
        "    matrix_output[i,0] = 1\n",
        "  elif pre_post_mean[\"post_groupings\"][i] == 'medium':\n",
        "    matrix_output[i,1] = 1\n",
        "  elif pre_post_mean[\"post_groupings\"][i] == 'high':\n",
        "    matrix_output[i,2] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "volEuzSSJIYo",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(matrix.shape[0]):\n",
        "  if pre_post_mean[\"groupings\"][i] =='low': #efficient\n",
        "    matrix[i,3] = 1\n",
        "  elif pre_post_mean[\"groupings\"][i] == 'medium':  \n",
        "    matrix[i,4] = 1\n",
        "  elif pre_post_mean[\"groupings\"][i] == 'high': #not efficient\n",
        "    matrix[i,5] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tVn4EkjxT-j-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(matrix_actions.shape[0]):\n",
        "  if pre_post_mean[\"groupings\"][i] =='low': #efficient\n",
        "    matrix_actions[i,0] = 1\n",
        "  elif pre_post_mean[\"groupings\"][i] == 'medium':  \n",
        "    matrix_actions[i,1] = 1\n",
        "  elif pre_post_mean[\"groupings\"][i] == 'high': #not efficient\n",
        "    matrix_actions[i,2] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "beZ9DBVrl2dN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for i in range(matrix_diff.shape[0]):\n",
        "  if pre_post_mean[\"differences\"][i] =='decrease': #this is wrong, fix it  \n",
        "    matrix_diff[i,0] = 1\n",
        "  elif pre_post_mean[\"differences\"][i] == 'no_change':\n",
        "    matrix_diff[i,1] = 1\n",
        "  elif pre_post_mean[\"differences\"][i] == 'increase':\n",
        "    matrix_diff[i,2] = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j_vJftP21vGJ",
        "colab_type": "code",
        "outputId": "f2d8ac4f-8339-495d-b9b5-3c70dbca8788",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "mean_array = pre_post_mean[\"groupings\"].values\n",
        "pre_array = pre_post_mean[\"pre_groupings\"].values\n",
        "post_array = pre_post_mean[\"post_groupings\"].values'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'\\nmean_array = pre_post_mean[\"groupings\"].values\\npre_array = pre_post_mean[\"pre_groupings\"].values\\npost_array = pre_post_mean[\"post_groupings\"].values'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 162
        }
      ]
    },
    {
      "metadata": {
        "id": "OI2OnJSfyh-R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# ***MODEL BUILDING***"
      ]
    },
    {
      "metadata": {
        "id": "WRdFM7vIBapU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import train_test_split\n",
        "pre_actions_train, pre_actions_test, post_train, post_test, pre_train, pre_test = train_test_split(matrix, matrix_output, matrix_pre_only, test_size=0.30, train_size=0.70)\n",
        "actions_train, actions_test = train_test_split(matrix_actions, test_size=0.30, train_size=0.70)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kk-7aq7mmIbY",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "diff_train, diff_test = train_test_split(matrix_diff, test_size=0.30, train_size=0.70)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HX4UwHJk-XRP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import keras\n",
        "from keras.layers import Input, Dense, LSTM, TimeDistributed, Lambda, multiply, Embedding, Activation, Flatten, Dropout\n",
        "from keras.models import Model, Sequential\n",
        "from keras.optimizers import RMSprop, Adam, SGD, Adamax\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras import backend as K\n",
        "import os\n",
        "from importlib import reload\n",
        "from sklearn import metrics\n",
        "from keras import backend as K\n",
        "import tensorflow as tf"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "_VvSq-kL2jhT",
        "colab_type": "code",
        "outputId": "226a1474-4d06-4deb-a8ff-56c680f07f28",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5244
        }
      },
      "cell_type": "code",
      "source": [
        "inputs = Input(shape=(3,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x = Dense(64, activation='relu')(inputs)\n",
        "x = Dense(64, activation='relu')(x)\n",
        "predictions = Dense(3, activation='softmax')(x)\n",
        "\n",
        "# This creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model = Model(inputs=inputs, outputs=predictions)\n",
        "model.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model.fit(pre_train, post_train, batch_size=16, epochs=150)  # starts training"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "171/171 [==============================] - 0s 3ms/step - loss: 1.0907 - acc: 0.4327\n",
            "Epoch 2/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0073 - acc: 0.7368\n",
            "Epoch 3/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9328 - acc: 0.7368\n",
            "Epoch 4/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.8682 - acc: 0.7368\n",
            "Epoch 5/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 0.8129 - acc: 0.7368\n",
            "Epoch 6/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.7623 - acc: 0.7368\n",
            "Epoch 7/150\n",
            "171/171 [==============================] - 0s 76us/step - loss: 0.7272 - acc: 0.7368\n",
            "Epoch 8/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 0.7085 - acc: 0.7368\n",
            "Epoch 9/150\n",
            "171/171 [==============================] - 0s 82us/step - loss: 0.6951 - acc: 0.7368\n",
            "Epoch 10/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.6886 - acc: 0.7368\n",
            "Epoch 11/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.6845 - acc: 0.7368\n",
            "Epoch 12/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.6800 - acc: 0.7368\n",
            "Epoch 13/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 0.6772 - acc: 0.7368\n",
            "Epoch 14/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6762 - acc: 0.7368\n",
            "Epoch 15/150\n",
            "171/171 [==============================] - 0s 112us/step - loss: 0.6764 - acc: 0.7368\n",
            "Epoch 16/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.6738 - acc: 0.7368\n",
            "Epoch 17/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.6741 - acc: 0.7368\n",
            "Epoch 18/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6764 - acc: 0.7368\n",
            "Epoch 19/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.6727 - acc: 0.7368\n",
            "Epoch 20/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6717 - acc: 0.7368\n",
            "Epoch 21/150\n",
            "171/171 [==============================] - 0s 125us/step - loss: 0.6732 - acc: 0.7368\n",
            "Epoch 22/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6729 - acc: 0.7368\n",
            "Epoch 23/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.6732 - acc: 0.7368\n",
            "Epoch 24/150\n",
            "171/171 [==============================] - 0s 84us/step - loss: 0.6717 - acc: 0.7368\n",
            "Epoch 25/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6715 - acc: 0.7368\n",
            "Epoch 26/150\n",
            "171/171 [==============================] - 0s 118us/step - loss: 0.6712 - acc: 0.7368\n",
            "Epoch 27/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.6707 - acc: 0.7368\n",
            "Epoch 28/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6719 - acc: 0.7368\n",
            "Epoch 29/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.6712 - acc: 0.7368\n",
            "Epoch 30/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6696 - acc: 0.7368\n",
            "Epoch 31/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.6714 - acc: 0.7368\n",
            "Epoch 32/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6699 - acc: 0.7368\n",
            "Epoch 33/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6720 - acc: 0.7368\n",
            "Epoch 34/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.6707 - acc: 0.7368\n",
            "Epoch 35/150\n",
            "171/171 [==============================] - 0s 114us/step - loss: 0.6698 - acc: 0.7368\n",
            "Epoch 36/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6740 - acc: 0.7368\n",
            "Epoch 37/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 0.6718 - acc: 0.7368\n",
            "Epoch 38/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 0.6702 - acc: 0.7368\n",
            "Epoch 39/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.6709 - acc: 0.7368\n",
            "Epoch 40/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.6691 - acc: 0.7368\n",
            "Epoch 41/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.6701 - acc: 0.7368\n",
            "Epoch 42/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.6700 - acc: 0.7368\n",
            "Epoch 43/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 0.6710 - acc: 0.7368\n",
            "Epoch 44/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.6720 - acc: 0.7368\n",
            "Epoch 45/150\n",
            "171/171 [==============================] - 0s 118us/step - loss: 0.6699 - acc: 0.7368\n",
            "Epoch 46/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.6695 - acc: 0.7368\n",
            "Epoch 47/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6701 - acc: 0.7368\n",
            "Epoch 48/150\n",
            "171/171 [==============================] - 0s 84us/step - loss: 0.6705 - acc: 0.7368\n",
            "Epoch 49/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.6737 - acc: 0.7368\n",
            "Epoch 50/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6707 - acc: 0.7368\n",
            "Epoch 51/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6699 - acc: 0.7368\n",
            "Epoch 52/150\n",
            "171/171 [==============================] - 0s 82us/step - loss: 0.6710 - acc: 0.7368\n",
            "Epoch 53/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6709 - acc: 0.7368\n",
            "Epoch 54/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6707 - acc: 0.7368\n",
            "Epoch 55/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.6705 - acc: 0.7368\n",
            "Epoch 56/150\n",
            "171/171 [==============================] - 0s 130us/step - loss: 0.6719 - acc: 0.7368\n",
            "Epoch 57/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.6738 - acc: 0.7368\n",
            "Epoch 58/150\n",
            "171/171 [==============================] - 0s 113us/step - loss: 0.6712 - acc: 0.7368\n",
            "Epoch 59/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6729 - acc: 0.7368\n",
            "Epoch 60/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.6737 - acc: 0.7368\n",
            "Epoch 61/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.6690 - acc: 0.7368\n",
            "Epoch 62/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6692 - acc: 0.7368\n",
            "Epoch 63/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6716 - acc: 0.7368\n",
            "Epoch 64/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6703 - acc: 0.7368\n",
            "Epoch 65/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.6711 - acc: 0.7368\n",
            "Epoch 66/150\n",
            "171/171 [==============================] - 0s 111us/step - loss: 0.6698 - acc: 0.7368\n",
            "Epoch 67/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.6705 - acc: 0.7368\n",
            "Epoch 68/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.6701 - acc: 0.7368\n",
            "Epoch 69/150\n",
            "171/171 [==============================] - 0s 109us/step - loss: 0.6690 - acc: 0.7368\n",
            "Epoch 70/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6690 - acc: 0.7368\n",
            "Epoch 71/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 0.6691 - acc: 0.7368\n",
            "Epoch 72/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.6703 - acc: 0.7368\n",
            "Epoch 73/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6690 - acc: 0.7368\n",
            "Epoch 74/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6712 - acc: 0.7368\n",
            "Epoch 75/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.6743 - acc: 0.7368\n",
            "Epoch 76/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.6692 - acc: 0.7368\n",
            "Epoch 77/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.6716 - acc: 0.7368\n",
            "Epoch 78/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.6702 - acc: 0.7368\n",
            "Epoch 79/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6693 - acc: 0.7368\n",
            "Epoch 80/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.6728 - acc: 0.7368\n",
            "Epoch 81/150\n",
            "171/171 [==============================] - 0s 119us/step - loss: 0.6701 - acc: 0.7368\n",
            "Epoch 82/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6699 - acc: 0.7368\n",
            "Epoch 83/150\n",
            "171/171 [==============================] - 0s 84us/step - loss: 0.6712 - acc: 0.7368\n",
            "Epoch 84/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6696 - acc: 0.7368\n",
            "Epoch 85/150\n",
            "171/171 [==============================] - 0s 84us/step - loss: 0.6700 - acc: 0.7368\n",
            "Epoch 86/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6706 - acc: 0.7368\n",
            "Epoch 87/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.6691 - acc: 0.7368\n",
            "Epoch 88/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6715 - acc: 0.7368\n",
            "Epoch 89/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6691 - acc: 0.7368\n",
            "Epoch 90/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.6725 - acc: 0.7368\n",
            "Epoch 91/150\n",
            "171/171 [==============================] - 0s 84us/step - loss: 0.6723 - acc: 0.7368\n",
            "Epoch 92/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 0.6705 - acc: 0.7368\n",
            "Epoch 93/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 0.6759 - acc: 0.7368\n",
            "Epoch 94/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.6729 - acc: 0.7368\n",
            "Epoch 95/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.6691 - acc: 0.7368\n",
            "Epoch 96/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.6700 - acc: 0.7368\n",
            "Epoch 97/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.6695 - acc: 0.7368\n",
            "Epoch 98/150\n",
            "171/171 [==============================] - 0s 111us/step - loss: 0.6707 - acc: 0.7368\n",
            "Epoch 99/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6695 - acc: 0.7368\n",
            "Epoch 100/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.6713 - acc: 0.7368\n",
            "Epoch 101/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.6702 - acc: 0.7368\n",
            "Epoch 102/150\n",
            "171/171 [==============================] - 0s 115us/step - loss: 0.6685 - acc: 0.7368\n",
            "Epoch 103/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.6691 - acc: 0.7368\n",
            "Epoch 104/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.6696 - acc: 0.7368\n",
            "Epoch 105/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 0.6711 - acc: 0.7368\n",
            "Epoch 106/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6712 - acc: 0.7368\n",
            "Epoch 107/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6694 - acc: 0.7368\n",
            "Epoch 108/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6702 - acc: 0.7368\n",
            "Epoch 109/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6712 - acc: 0.7368\n",
            "Epoch 110/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6697 - acc: 0.7368\n",
            "Epoch 111/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6692 - acc: 0.7368\n",
            "Epoch 112/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.6702 - acc: 0.7368\n",
            "Epoch 113/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 0.6703 - acc: 0.7368\n",
            "Epoch 114/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.6705 - acc: 0.7368\n",
            "Epoch 115/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6686 - acc: 0.7368\n",
            "Epoch 116/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 0.6686 - acc: 0.7368\n",
            "Epoch 117/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6698 - acc: 0.7368\n",
            "Epoch 118/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.6691 - acc: 0.7368\n",
            "Epoch 119/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6702 - acc: 0.7368\n",
            "Epoch 120/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.6707 - acc: 0.7368\n",
            "Epoch 121/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6693 - acc: 0.7368\n",
            "Epoch 122/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 0.6689 - acc: 0.7368\n",
            "Epoch 123/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 0.6703 - acc: 0.7368\n",
            "Epoch 124/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6709 - acc: 0.7368\n",
            "Epoch 125/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.6689 - acc: 0.7368\n",
            "Epoch 126/150\n",
            "171/171 [==============================] - 0s 115us/step - loss: 0.6688 - acc: 0.7368\n",
            "Epoch 127/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 0.6691 - acc: 0.7368\n",
            "Epoch 128/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6706 - acc: 0.7368\n",
            "Epoch 129/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.6692 - acc: 0.7368\n",
            "Epoch 130/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6709 - acc: 0.7368\n",
            "Epoch 131/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 0.6689 - acc: 0.7368\n",
            "Epoch 132/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.6710 - acc: 0.7368\n",
            "Epoch 133/150\n",
            "171/171 [==============================] - 0s 83us/step - loss: 0.6703 - acc: 0.7368\n",
            "Epoch 134/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.6702 - acc: 0.7368\n",
            "Epoch 135/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6688 - acc: 0.7368\n",
            "Epoch 136/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.6689 - acc: 0.7368\n",
            "Epoch 137/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 0.6719 - acc: 0.7368\n",
            "Epoch 138/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.6702 - acc: 0.7368\n",
            "Epoch 139/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6684 - acc: 0.7368\n",
            "Epoch 140/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6704 - acc: 0.7368\n",
            "Epoch 141/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.6689 - acc: 0.7368\n",
            "Epoch 142/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.6720 - acc: 0.7368\n",
            "Epoch 143/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 0.6694 - acc: 0.7368\n",
            "Epoch 144/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 0.6693 - acc: 0.7368\n",
            "Epoch 145/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 0.6692 - acc: 0.7368\n",
            "Epoch 146/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6685 - acc: 0.7368\n",
            "Epoch 147/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.6699 - acc: 0.7368\n",
            "Epoch 148/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.6688 - acc: 0.7368\n",
            "Epoch 149/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.6699 - acc: 0.7368\n",
            "Epoch 150/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.6707 - acc: 0.7368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbb6c7194e0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "metadata": {
        "id": "SAiDuqTpPIpQ",
        "colab_type": "code",
        "outputId": "5a6ce096-7098-47cc-ddb4-b6bc91623dc0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "scores = model.evaluate(pre_test, post_test, batch_size=16)\n",
        "print(\"\\n%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74/74 [==============================] - 0s 2ms/step\n",
            "\n",
            "acc: 64.86%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xzxi4V-4LtZv",
        "colab_type": "code",
        "outputId": "87b7bcd3-8939-410a-f272-04f73f0fbef9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5244
        }
      },
      "cell_type": "code",
      "source": [
        "inputs1 = Input(shape=(6,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x1 = Dense(64, activation='relu')(inputs1)\n",
        "x1 = Dense(64, activation='relu')(x1)\n",
        "predictions1 = Dense(3, activation='softmax')(x1)\n",
        "\n",
        "# This creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model1 = Model(inputs=inputs1, outputs=predictions1)\n",
        "model1.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model1.fit(pre_actions_train, post_train, batch_size=16, epochs=150)  # starts training"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "171/171 [==============================] - 1s 3ms/step - loss: 1.0436 - acc: 0.5614\n",
            "Epoch 2/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.9552 - acc: 0.6608\n",
            "Epoch 3/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.8876 - acc: 0.6842\n",
            "Epoch 4/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.8191 - acc: 0.6901\n",
            "Epoch 5/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.7663 - acc: 0.7135\n",
            "Epoch 6/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.7230 - acc: 0.7368\n",
            "Epoch 7/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 0.6988 - acc: 0.7368\n",
            "Epoch 8/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6836 - acc: 0.7368\n",
            "Epoch 9/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 0.6805 - acc: 0.7368\n",
            "Epoch 10/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 0.6749 - acc: 0.7368\n",
            "Epoch 11/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.6727 - acc: 0.7368\n",
            "Epoch 12/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6702 - acc: 0.7368\n",
            "Epoch 13/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6703 - acc: 0.7368\n",
            "Epoch 14/150\n",
            "171/171 [==============================] - 0s 119us/step - loss: 0.6677 - acc: 0.7368\n",
            "Epoch 15/150\n",
            "171/171 [==============================] - 0s 122us/step - loss: 0.6663 - acc: 0.7368\n",
            "Epoch 16/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6654 - acc: 0.7368\n",
            "Epoch 17/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6638 - acc: 0.7368\n",
            "Epoch 18/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.6644 - acc: 0.7368\n",
            "Epoch 19/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.6638 - acc: 0.7368\n",
            "Epoch 20/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.6623 - acc: 0.7368\n",
            "Epoch 21/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.6648 - acc: 0.7368\n",
            "Epoch 22/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6629 - acc: 0.7368\n",
            "Epoch 23/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6622 - acc: 0.7368\n",
            "Epoch 24/150\n",
            "171/171 [==============================] - 0s 109us/step - loss: 0.6592 - acc: 0.7368\n",
            "Epoch 25/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6606 - acc: 0.7368\n",
            "Epoch 26/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.6605 - acc: 0.7368\n",
            "Epoch 27/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.6624 - acc: 0.7368\n",
            "Epoch 28/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.6592 - acc: 0.7368\n",
            "Epoch 29/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6606 - acc: 0.7368\n",
            "Epoch 30/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 0.6598 - acc: 0.7368\n",
            "Epoch 31/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6615 - acc: 0.7368\n",
            "Epoch 32/150\n",
            "171/171 [==============================] - 0s 132us/step - loss: 0.6599 - acc: 0.7368\n",
            "Epoch 33/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.6586 - acc: 0.7368\n",
            "Epoch 34/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 0.6581 - acc: 0.7368\n",
            "Epoch 35/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.6591 - acc: 0.7368\n",
            "Epoch 36/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6581 - acc: 0.7368\n",
            "Epoch 37/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.6580 - acc: 0.7368\n",
            "Epoch 38/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.6574 - acc: 0.7368\n",
            "Epoch 39/150\n",
            "171/171 [==============================] - 0s 109us/step - loss: 0.6564 - acc: 0.7368\n",
            "Epoch 40/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.6593 - acc: 0.7368\n",
            "Epoch 41/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.6598 - acc: 0.7368\n",
            "Epoch 42/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.6608 - acc: 0.7368\n",
            "Epoch 43/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.6567 - acc: 0.7368\n",
            "Epoch 44/150\n",
            "171/171 [==============================] - 0s 83us/step - loss: 0.6570 - acc: 0.7368\n",
            "Epoch 45/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6564 - acc: 0.7368\n",
            "Epoch 46/150\n",
            "171/171 [==============================] - 0s 111us/step - loss: 0.6546 - acc: 0.7368\n",
            "Epoch 47/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.6561 - acc: 0.7368\n",
            "Epoch 48/150\n",
            "171/171 [==============================] - 0s 80us/step - loss: 0.6600 - acc: 0.7368\n",
            "Epoch 49/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 0.6577 - acc: 0.7368\n",
            "Epoch 50/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.6568 - acc: 0.7368\n",
            "Epoch 51/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.6559 - acc: 0.7368\n",
            "Epoch 52/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6554 - acc: 0.7368\n",
            "Epoch 53/150\n",
            "171/171 [==============================] - 0s 111us/step - loss: 0.6579 - acc: 0.7368\n",
            "Epoch 54/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6562 - acc: 0.7368\n",
            "Epoch 55/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.6566 - acc: 0.7368\n",
            "Epoch 56/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.6561 - acc: 0.7368\n",
            "Epoch 57/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.6545 - acc: 0.7368\n",
            "Epoch 58/150\n",
            "171/171 [==============================] - 0s 126us/step - loss: 0.6560 - acc: 0.7368\n",
            "Epoch 59/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.6554 - acc: 0.7368\n",
            "Epoch 60/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.6557 - acc: 0.7368\n",
            "Epoch 61/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6567 - acc: 0.7368\n",
            "Epoch 62/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.6546 - acc: 0.7368\n",
            "Epoch 63/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6548 - acc: 0.7368\n",
            "Epoch 64/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.6547 - acc: 0.7368\n",
            "Epoch 65/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.6547 - acc: 0.7368\n",
            "Epoch 66/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6576 - acc: 0.7368\n",
            "Epoch 67/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.6554 - acc: 0.7368\n",
            "Epoch 68/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6561 - acc: 0.7368\n",
            "Epoch 69/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6548 - acc: 0.7368\n",
            "Epoch 70/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6543 - acc: 0.7368\n",
            "Epoch 71/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.6550 - acc: 0.7368\n",
            "Epoch 72/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6583 - acc: 0.7368\n",
            "Epoch 73/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6541 - acc: 0.7368\n",
            "Epoch 74/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.6558 - acc: 0.7368\n",
            "Epoch 75/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6543 - acc: 0.7368\n",
            "Epoch 76/150\n",
            "171/171 [==============================] - 0s 117us/step - loss: 0.6559 - acc: 0.7368\n",
            "Epoch 77/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6558 - acc: 0.7368\n",
            "Epoch 78/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.6568 - acc: 0.7368\n",
            "Epoch 79/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6560 - acc: 0.7368\n",
            "Epoch 80/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 0.6562 - acc: 0.7368\n",
            "Epoch 81/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.6554 - acc: 0.7368\n",
            "Epoch 82/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6586 - acc: 0.7368\n",
            "Epoch 83/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.6576 - acc: 0.7368\n",
            "Epoch 84/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6547 - acc: 0.7368\n",
            "Epoch 85/150\n",
            "171/171 [==============================] - 0s 120us/step - loss: 0.6549 - acc: 0.7368\n",
            "Epoch 86/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.6538 - acc: 0.7368\n",
            "Epoch 87/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6572 - acc: 0.7368\n",
            "Epoch 88/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 0.6535 - acc: 0.7368\n",
            "Epoch 89/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6575 - acc: 0.7368\n",
            "Epoch 90/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6608 - acc: 0.7368\n",
            "Epoch 91/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6550 - acc: 0.7368\n",
            "Epoch 92/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6557 - acc: 0.7368\n",
            "Epoch 93/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6546 - acc: 0.7368\n",
            "Epoch 94/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6550 - acc: 0.7368\n",
            "Epoch 95/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.6538 - acc: 0.7368\n",
            "Epoch 96/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 0.6542 - acc: 0.7368\n",
            "Epoch 97/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.6535 - acc: 0.7368\n",
            "Epoch 98/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6546 - acc: 0.7368\n",
            "Epoch 99/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.6542 - acc: 0.7368\n",
            "Epoch 100/150\n",
            "171/171 [==============================] - 0s 81us/step - loss: 0.6553 - acc: 0.7368\n",
            "Epoch 101/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.6551 - acc: 0.7368\n",
            "Epoch 102/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6562 - acc: 0.7368\n",
            "Epoch 103/150\n",
            "171/171 [==============================] - 0s 114us/step - loss: 0.6567 - acc: 0.7368\n",
            "Epoch 104/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6544 - acc: 0.7368\n",
            "Epoch 105/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.6535 - acc: 0.7368\n",
            "Epoch 106/150\n",
            "171/171 [==============================] - 0s 83us/step - loss: 0.6563 - acc: 0.7368\n",
            "Epoch 107/150\n",
            "171/171 [==============================] - 0s 234us/step - loss: 0.6543 - acc: 0.7368\n",
            "Epoch 108/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.6532 - acc: 0.7368\n",
            "Epoch 109/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6537 - acc: 0.7368\n",
            "Epoch 110/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.6539 - acc: 0.7368\n",
            "Epoch 111/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6557 - acc: 0.7368\n",
            "Epoch 112/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6584 - acc: 0.7368\n",
            "Epoch 113/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.6528 - acc: 0.7368\n",
            "Epoch 114/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6528 - acc: 0.7368\n",
            "Epoch 115/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.6543 - acc: 0.7368\n",
            "Epoch 116/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6545 - acc: 0.7368\n",
            "Epoch 117/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6544 - acc: 0.7368\n",
            "Epoch 118/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6582 - acc: 0.7368\n",
            "Epoch 119/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6537 - acc: 0.7368\n",
            "Epoch 120/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6545 - acc: 0.7368\n",
            "Epoch 121/150\n",
            "171/171 [==============================] - 0s 121us/step - loss: 0.6545 - acc: 0.7368\n",
            "Epoch 122/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6536 - acc: 0.7368\n",
            "Epoch 123/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6526 - acc: 0.7368\n",
            "Epoch 124/150\n",
            "171/171 [==============================] - 0s 118us/step - loss: 0.6542 - acc: 0.7368\n",
            "Epoch 125/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.6541 - acc: 0.7368\n",
            "Epoch 126/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6539 - acc: 0.7368\n",
            "Epoch 127/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6537 - acc: 0.7368\n",
            "Epoch 128/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.6549 - acc: 0.7368\n",
            "Epoch 129/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.6551 - acc: 0.7368\n",
            "Epoch 130/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6533 - acc: 0.7368\n",
            "Epoch 131/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.6546 - acc: 0.7368\n",
            "Epoch 132/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.6559 - acc: 0.7368\n",
            "Epoch 133/150\n",
            "171/171 [==============================] - 0s 83us/step - loss: 0.6535 - acc: 0.7368\n",
            "Epoch 134/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 0.6551 - acc: 0.7368\n",
            "Epoch 135/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.6556 - acc: 0.7368\n",
            "Epoch 136/150\n",
            "171/171 [==============================] - 0s 132us/step - loss: 0.6531 - acc: 0.7368\n",
            "Epoch 137/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.6552 - acc: 0.7368\n",
            "Epoch 138/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6540 - acc: 0.7368\n",
            "Epoch 139/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 0.6552 - acc: 0.7368\n",
            "Epoch 140/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6571 - acc: 0.7368\n",
            "Epoch 141/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.6546 - acc: 0.7368\n",
            "Epoch 142/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.6554 - acc: 0.7368\n",
            "Epoch 143/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.6534 - acc: 0.7368\n",
            "Epoch 144/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.6529 - acc: 0.7368\n",
            "Epoch 145/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 0.6527 - acc: 0.7368\n",
            "Epoch 146/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.6550 - acc: 0.7368\n",
            "Epoch 147/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.6582 - acc: 0.7368\n",
            "Epoch 148/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 0.6554 - acc: 0.7368\n",
            "Epoch 149/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.6524 - acc: 0.7368\n",
            "Epoch 150/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 0.6541 - acc: 0.7368\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbb6bd87668>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 168
        }
      ]
    },
    {
      "metadata": {
        "id": "ekCTLYoUdDMa",
        "colab_type": "code",
        "outputId": "c39ce9e6-f070-4ddc-a46e-0795e1865234",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "cell_type": "code",
      "source": [
        "'''\n",
        "inputs1 = Input(shape=(3*2,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x1 = Dense(64, activation='relu')(inputs1)\n",
        "x1 = Dense(64, activation='relu')(x1)\n",
        "predictions1 = Dense(3, activation='softmax')(x1)\n",
        "\n",
        "# This creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model1 = Model(inputs=inputs1, outputs=predictions1)\n",
        "model1.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model1.fit([[pre_train, actions_train]], post_train, batch_size=16, epochs=150)  # starts training'''"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"\\ninputs1 = Input(shape=(3*2,))\\n\\n# a layer instance is callable on a tensor, and returns a tensor\\nx1 = Dense(64, activation='relu')(inputs1)\\nx1 = Dense(64, activation='relu')(x1)\\npredictions1 = Dense(3, activation='softmax')(x1)\\n\\n# This creates a model that includes\\n# the Input layer and three Dense layers\\nmodel1 = Model(inputs=inputs1, outputs=predictions1)\\nmodel1.compile(optimizer='adam',\\n              loss='categorical_crossentropy',\\n              metrics=['accuracy'])\\nmodel1.fit([[pre_train, actions_train]], post_train, batch_size=16, epochs=150)  # starts training\""
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 169
        }
      ]
    },
    {
      "metadata": {
        "id": "UW1e1XULP00i",
        "colab_type": "code",
        "outputId": "47e1631a-417c-4e3c-a0a7-4f0091853808",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "scores1 = model1.evaluate(pre_actions_test, post_test, batch_size=16)\n",
        "print(\"\\n%s: %.2f%%\" % (model1.metrics_names[1], scores1[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74/74 [==============================] - 0s 3ms/step\n",
            "\n",
            "acc: 64.86%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "u9rOOaxvUaWe",
        "colab_type": "code",
        "outputId": "4b3e447c-6be1-4e48-c58b-0cd58cb71124",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5244
        }
      },
      "cell_type": "code",
      "source": [
        "inputs2 = Input(shape=(3,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x2 = Dense(64, activation='relu')(inputs2)\n",
        "x2 = Dense(64, activation='relu')(x2)\n",
        "predictions2 = Dense(3, activation='softmax')(x2)\n",
        "\n",
        "# This creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model2 = Model(inputs=inputs2, outputs=predictions2)\n",
        "model2.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model2.fit(actions_train, post_train, batch_size=16, epochs=150)  # starts training"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "171/171 [==============================] - 1s 3ms/step - loss: 1.0868 - acc: 0.4327\n",
            "Epoch 2/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0452 - acc: 0.5088\n",
            "Epoch 3/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0310 - acc: 0.5088\n",
            "Epoch 4/150\n",
            "171/171 [==============================] - 0s 81us/step - loss: 1.0273 - acc: 0.5088\n",
            "Epoch 5/150\n",
            "171/171 [==============================] - 0s 116us/step - loss: 1.0277 - acc: 0.5088\n",
            "Epoch 6/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0266 - acc: 0.5088\n",
            "Epoch 7/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0267 - acc: 0.5088\n",
            "Epoch 8/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 1.0277 - acc: 0.5088\n",
            "Epoch 9/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0266 - acc: 0.5088\n",
            "Epoch 10/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0267 - acc: 0.5088\n",
            "Epoch 11/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 1.0262 - acc: 0.5088\n",
            "Epoch 12/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 1.0262 - acc: 0.5088\n",
            "Epoch 13/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0262 - acc: 0.5088\n",
            "Epoch 14/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 1.0246 - acc: 0.5088\n",
            "Epoch 15/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 1.0264 - acc: 0.5088\n",
            "Epoch 16/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0251 - acc: 0.5088\n",
            "Epoch 17/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0252 - acc: 0.5088\n",
            "Epoch 18/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0246 - acc: 0.5088\n",
            "Epoch 19/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 1.0257 - acc: 0.5088\n",
            "Epoch 20/150\n",
            "171/171 [==============================] - 0s 83us/step - loss: 1.0244 - acc: 0.5088\n",
            "Epoch 21/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0255 - acc: 0.5088\n",
            "Epoch 22/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0257 - acc: 0.5088\n",
            "Epoch 23/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 1.0242 - acc: 0.5088\n",
            "Epoch 24/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0255 - acc: 0.5088\n",
            "Epoch 25/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0241 - acc: 0.5088\n",
            "Epoch 26/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0247 - acc: 0.5088\n",
            "Epoch 27/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0252 - acc: 0.5088\n",
            "Epoch 28/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 1.0242 - acc: 0.5088\n",
            "Epoch 29/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0241 - acc: 0.5088\n",
            "Epoch 30/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0245 - acc: 0.5088\n",
            "Epoch 31/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 1.0256 - acc: 0.5088\n",
            "Epoch 32/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0242 - acc: 0.5088\n",
            "Epoch 33/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 1.0239 - acc: 0.5088\n",
            "Epoch 34/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 35/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 1.0244 - acc: 0.5088\n",
            "Epoch 36/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0240 - acc: 0.5088\n",
            "Epoch 37/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 1.0246 - acc: 0.5088\n",
            "Epoch 38/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 39/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0236 - acc: 0.5088\n",
            "Epoch 40/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 41/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0249 - acc: 0.5088\n",
            "Epoch 42/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0239 - acc: 0.5088\n",
            "Epoch 43/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0245 - acc: 0.5088\n",
            "Epoch 44/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 1.0239 - acc: 0.5088\n",
            "Epoch 45/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 1.0247 - acc: 0.5088\n",
            "Epoch 46/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0236 - acc: 0.5088\n",
            "Epoch 47/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 48/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 1.0248 - acc: 0.5088\n",
            "Epoch 49/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 50/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 1.0239 - acc: 0.5088\n",
            "Epoch 51/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0242 - acc: 0.5088\n",
            "Epoch 52/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0242 - acc: 0.5088\n",
            "Epoch 53/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0247 - acc: 0.5088\n",
            "Epoch 54/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0241 - acc: 0.5088\n",
            "Epoch 55/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0256 - acc: 0.5088\n",
            "Epoch 56/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0276 - acc: 0.5088\n",
            "Epoch 57/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0243 - acc: 0.5088\n",
            "Epoch 58/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 1.0241 - acc: 0.5088\n",
            "Epoch 59/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 60/150\n",
            "171/171 [==============================] - 0s 122us/step - loss: 1.0233 - acc: 0.5088\n",
            "Epoch 61/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0243 - acc: 0.5088\n",
            "Epoch 62/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 1.0249 - acc: 0.5088\n",
            "Epoch 63/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 1.0243 - acc: 0.5088\n",
            "Epoch 64/150\n",
            "171/171 [==============================] - 0s 78us/step - loss: 1.0242 - acc: 0.5088\n",
            "Epoch 65/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0258 - acc: 0.5088\n",
            "Epoch 66/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0240 - acc: 0.5088\n",
            "Epoch 67/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 1.0245 - acc: 0.5088\n",
            "Epoch 68/150\n",
            "171/171 [==============================] - 0s 76us/step - loss: 1.0236 - acc: 0.5088\n",
            "Epoch 69/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 1.0247 - acc: 0.5088\n",
            "Epoch 70/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0247 - acc: 0.5088\n",
            "Epoch 71/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0243 - acc: 0.5088\n",
            "Epoch 72/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 1.0258 - acc: 0.5088\n",
            "Epoch 73/150\n",
            "171/171 [==============================] - 0s 116us/step - loss: 1.0269 - acc: 0.5088\n",
            "Epoch 74/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 1.0257 - acc: 0.5088\n",
            "Epoch 75/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0236 - acc: 0.5088\n",
            "Epoch 76/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0247 - acc: 0.5088\n",
            "Epoch 77/150\n",
            "171/171 [==============================] - 0s 117us/step - loss: 1.0236 - acc: 0.5088\n",
            "Epoch 78/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 1.0239 - acc: 0.5088\n",
            "Epoch 79/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 80/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0240 - acc: 0.5088\n",
            "Epoch 81/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0239 - acc: 0.5088\n",
            "Epoch 82/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0241 - acc: 0.5088\n",
            "Epoch 83/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0241 - acc: 0.5088\n",
            "Epoch 84/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 1.0259 - acc: 0.5088\n",
            "Epoch 85/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 86/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 1.0244 - acc: 0.5088\n",
            "Epoch 87/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 1.0244 - acc: 0.5088\n",
            "Epoch 88/150\n",
            "171/171 [==============================] - 0s 84us/step - loss: 1.0251 - acc: 0.5088\n",
            "Epoch 89/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0239 - acc: 0.5088\n",
            "Epoch 90/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 1.0250 - acc: 0.5088\n",
            "Epoch 91/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 1.0241 - acc: 0.5088\n",
            "Epoch 92/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 1.0256 - acc: 0.5088\n",
            "Epoch 93/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0239 - acc: 0.5088\n",
            "Epoch 94/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0242 - acc: 0.5088\n",
            "Epoch 95/150\n",
            "171/171 [==============================] - 0s 81us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 96/150\n",
            "171/171 [==============================] - 0s 114us/step - loss: 1.0246 - acc: 0.5088\n",
            "Epoch 97/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0236 - acc: 0.5088\n",
            "Epoch 98/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 99/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 1.0247 - acc: 0.5088\n",
            "Epoch 100/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0240 - acc: 0.5088\n",
            "Epoch 101/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0236 - acc: 0.5088\n",
            "Epoch 102/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0254 - acc: 0.5088\n",
            "Epoch 103/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0229 - acc: 0.5088\n",
            "Epoch 104/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0237 - acc: 0.5088\n",
            "Epoch 105/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 1.0241 - acc: 0.5088\n",
            "Epoch 106/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 1.0237 - acc: 0.5088\n",
            "Epoch 107/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0278 - acc: 0.5088\n",
            "Epoch 108/150\n",
            "171/171 [==============================] - 0s 81us/step - loss: 1.0240 - acc: 0.5088\n",
            "Epoch 109/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 1.0234 - acc: 0.5088\n",
            "Epoch 110/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0239 - acc: 0.5088\n",
            "Epoch 111/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0241 - acc: 0.5088\n",
            "Epoch 112/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0235 - acc: 0.5088\n",
            "Epoch 113/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 114/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0235 - acc: 0.5088\n",
            "Epoch 115/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 1.0237 - acc: 0.5088\n",
            "Epoch 116/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0242 - acc: 0.5088\n",
            "Epoch 117/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0241 - acc: 0.5088\n",
            "Epoch 118/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 119/150\n",
            "171/171 [==============================] - 0s 109us/step - loss: 1.0239 - acc: 0.5088\n",
            "Epoch 120/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 1.0250 - acc: 0.5088\n",
            "Epoch 121/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0240 - acc: 0.5088\n",
            "Epoch 122/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0237 - acc: 0.5088\n",
            "Epoch 123/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 124/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0243 - acc: 0.5088\n",
            "Epoch 125/150\n",
            "171/171 [==============================] - 0s 82us/step - loss: 1.0264 - acc: 0.5088\n",
            "Epoch 126/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0249 - acc: 0.5088\n",
            "Epoch 127/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0245 - acc: 0.5088\n",
            "Epoch 128/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0244 - acc: 0.5088\n",
            "Epoch 129/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0239 - acc: 0.5088\n",
            "Epoch 130/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0235 - acc: 0.5088\n",
            "Epoch 131/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0254 - acc: 0.5088\n",
            "Epoch 132/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 1.0240 - acc: 0.5088\n",
            "Epoch 133/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 1.0240 - acc: 0.5088\n",
            "Epoch 134/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0249 - acc: 0.5088\n",
            "Epoch 135/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 136/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 1.0241 - acc: 0.5088\n",
            "Epoch 137/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 1.0235 - acc: 0.5088\n",
            "Epoch 138/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 1.0249 - acc: 0.5088\n",
            "Epoch 139/150\n",
            "171/171 [==============================] - 0s 84us/step - loss: 1.0235 - acc: 0.5088\n",
            "Epoch 140/150\n",
            "171/171 [==============================] - 0s 84us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 141/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 1.0235 - acc: 0.5088\n",
            "Epoch 142/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0235 - acc: 0.5088\n",
            "Epoch 143/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0243 - acc: 0.5088\n",
            "Epoch 144/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 1.0239 - acc: 0.5088\n",
            "Epoch 145/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 1.0238 - acc: 0.5088\n",
            "Epoch 146/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 1.0246 - acc: 0.5088\n",
            "Epoch 147/150\n",
            "171/171 [==============================] - 0s 112us/step - loss: 1.0233 - acc: 0.5088\n",
            "Epoch 148/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0244 - acc: 0.5088\n",
            "Epoch 149/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0235 - acc: 0.5088\n",
            "Epoch 150/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0238 - acc: 0.5088\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbb6c7ecb00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 171
        }
      ]
    },
    {
      "metadata": {
        "id": "FZNmOU4_Ui2p",
        "colab_type": "code",
        "outputId": "eb773657-5c4d-477a-fe1b-3bb78d1ef312",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "scores2 = model2.evaluate(actions_test, post_test, batch_size=16)\n",
        "print(\"\\n%s: %.2f%%\" % (model2.metrics_names[1], scores2[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74/74 [==============================] - 0s 3ms/step\n",
            "\n",
            "acc: 44.59%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f5N9Y0hymSS4",
        "colab_type": "code",
        "outputId": "b3be5633-d792-41f0-8728-8a16a3b6d650",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5244
        }
      },
      "cell_type": "code",
      "source": [
        "inputs6 = Input(shape=(3,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x6 = Dense(64, activation='relu')(inputs6)\n",
        "x6 = Dense(64, activation='relu')(x6)\n",
        "predictions6 = Dense(3, activation='softmax')(x6)\n",
        "\n",
        "# This creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model6 = Model(inputs=inputs6, outputs=predictions6)\n",
        "model6.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model6.fit(pre_train, diff_train, batch_size=16, epochs=150)  # starts training"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "171/171 [==============================] - 1s 3ms/step - loss: 1.0947 - acc: 0.4094\n",
            "Epoch 2/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 1.0479 - acc: 0.4152\n",
            "Epoch 3/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0269 - acc: 0.4269\n",
            "Epoch 4/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0138 - acc: 0.4503\n",
            "Epoch 5/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0109 - acc: 0.4503\n",
            "Epoch 6/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0099 - acc: 0.4503\n",
            "Epoch 7/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0059 - acc: 0.4503\n",
            "Epoch 8/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0050 - acc: 0.4503\n",
            "Epoch 9/150\n",
            "171/171 [==============================] - 0s 115us/step - loss: 1.0035 - acc: 0.4503\n",
            "Epoch 10/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 1.0035 - acc: 0.4503\n",
            "Epoch 11/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0034 - acc: 0.4503\n",
            "Epoch 12/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 1.0024 - acc: 0.4503\n",
            "Epoch 13/150\n",
            "171/171 [==============================] - 0s 123us/step - loss: 1.0055 - acc: 0.3801\n",
            "Epoch 14/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 1.0034 - acc: 0.4503\n",
            "Epoch 15/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0027 - acc: 0.4503\n",
            "Epoch 16/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 1.0046 - acc: 0.4503\n",
            "Epoch 17/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0019 - acc: 0.4503\n",
            "Epoch 18/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0023 - acc: 0.4503\n",
            "Epoch 19/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0032 - acc: 0.4503\n",
            "Epoch 20/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 1.0016 - acc: 0.4503\n",
            "Epoch 21/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0032 - acc: 0.4503\n",
            "Epoch 22/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0022 - acc: 0.4503\n",
            "Epoch 23/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0030 - acc: 0.4503\n",
            "Epoch 24/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 1.0024 - acc: 0.4503\n",
            "Epoch 25/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0032 - acc: 0.4503\n",
            "Epoch 26/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 1.0032 - acc: 0.4503\n",
            "Epoch 27/150\n",
            "171/171 [==============================] - 0s 109us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 28/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0028 - acc: 0.4503\n",
            "Epoch 29/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0022 - acc: 0.4503\n",
            "Epoch 30/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 1.0019 - acc: 0.4503\n",
            "Epoch 31/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0016 - acc: 0.4503\n",
            "Epoch 32/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0019 - acc: 0.4503\n",
            "Epoch 33/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 34/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 1.0039 - acc: 0.4503\n",
            "Epoch 35/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0013 - acc: 0.4503\n",
            "Epoch 36/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 37/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 38/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0019 - acc: 0.4503\n",
            "Epoch 39/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0012 - acc: 0.4503\n",
            "Epoch 40/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 1.0015 - acc: 0.4503\n",
            "Epoch 41/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 1.0027 - acc: 0.4503\n",
            "Epoch 42/150\n",
            "171/171 [==============================] - 0s 123us/step - loss: 1.0027 - acc: 0.4503\n",
            "Epoch 43/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 1.0013 - acc: 0.4503\n",
            "Epoch 44/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 45/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 46/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 47/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 1.0022 - acc: 0.4503\n",
            "Epoch 48/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 1.0019 - acc: 0.4503\n",
            "Epoch 49/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 1.0022 - acc: 0.4503\n",
            "Epoch 50/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0015 - acc: 0.4503\n",
            "Epoch 51/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0022 - acc: 0.4503\n",
            "Epoch 52/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 1.0015 - acc: 0.4503\n",
            "Epoch 53/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 54/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 1.0015 - acc: 0.4503\n",
            "Epoch 55/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 1.0016 - acc: 0.4503\n",
            "Epoch 56/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0023 - acc: 0.4503\n",
            "Epoch 57/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0019 - acc: 0.4503\n",
            "Epoch 58/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0017 - acc: 0.4503\n",
            "Epoch 59/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 1.0017 - acc: 0.4503\n",
            "Epoch 60/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 1.0019 - acc: 0.4503\n",
            "Epoch 61/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 1.0026 - acc: 0.4503\n",
            "Epoch 62/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 1.0022 - acc: 0.4503\n",
            "Epoch 63/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0017 - acc: 0.4503\n",
            "Epoch 64/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 65/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0019 - acc: 0.4503\n",
            "Epoch 66/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0025 - acc: 0.4503\n",
            "Epoch 67/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0023 - acc: 0.4503\n",
            "Epoch 68/150\n",
            "171/171 [==============================] - 0s 155us/step - loss: 1.0027 - acc: 0.4503\n",
            "Epoch 69/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0023 - acc: 0.4503\n",
            "Epoch 70/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 71/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 1.0015 - acc: 0.4503\n",
            "Epoch 72/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 73/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0029 - acc: 0.4503\n",
            "Epoch 74/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 75/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 76/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 77/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0035 - acc: 0.4503\n",
            "Epoch 78/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0016 - acc: 0.4503\n",
            "Epoch 79/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 80/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 1.0016 - acc: 0.4503\n",
            "Epoch 81/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0015 - acc: 0.4503\n",
            "Epoch 82/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 83/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0030 - acc: 0.4503\n",
            "Epoch 84/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 1.0025 - acc: 0.4503\n",
            "Epoch 85/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 1.0024 - acc: 0.4503\n",
            "Epoch 86/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 87/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0038 - acc: 0.4503\n",
            "Epoch 88/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 1.0026 - acc: 0.4503\n",
            "Epoch 89/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 90/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 91/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0022 - acc: 0.4503\n",
            "Epoch 92/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 93/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 1.0016 - acc: 0.4503\n",
            "Epoch 94/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 1.0025 - acc: 0.4503\n",
            "Epoch 95/150\n",
            "171/171 [==============================] - 0s 139us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 96/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 97/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 98/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0017 - acc: 0.4503\n",
            "Epoch 99/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0016 - acc: 0.4503\n",
            "Epoch 100/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 101/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 1.0023 - acc: 0.4503\n",
            "Epoch 102/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 103/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 1.0022 - acc: 0.4503\n",
            "Epoch 104/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 105/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0024 - acc: 0.4503\n",
            "Epoch 106/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0012 - acc: 0.4503\n",
            "Epoch 107/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 108/150\n",
            "171/171 [==============================] - 0s 109us/step - loss: 1.0017 - acc: 0.4503\n",
            "Epoch 109/150\n",
            "171/171 [==============================] - 0s 124us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 110/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 1.0032 - acc: 0.4503\n",
            "Epoch 111/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0015 - acc: 0.4503\n",
            "Epoch 112/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 113/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0032 - acc: 0.4503\n",
            "Epoch 114/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 115/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0013 - acc: 0.4503\n",
            "Epoch 116/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 1.0025 - acc: 0.4503\n",
            "Epoch 117/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0017 - acc: 0.4503\n",
            "Epoch 118/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 119/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 120/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0022 - acc: 0.4503\n",
            "Epoch 121/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 122/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 1.0019 - acc: 0.4503\n",
            "Epoch 123/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 1.0016 - acc: 0.4503\n",
            "Epoch 124/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0025 - acc: 0.4503\n",
            "Epoch 125/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 126/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0017 - acc: 0.4503\n",
            "Epoch 127/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0034 - acc: 0.4503\n",
            "Epoch 128/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0015 - acc: 0.4503\n",
            "Epoch 129/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 1.0024 - acc: 0.4503\n",
            "Epoch 130/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 131/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0025 - acc: 0.4503\n",
            "Epoch 132/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 1.0036 - acc: 0.4503\n",
            "Epoch 133/150\n",
            "171/171 [==============================] - 0s 83us/step - loss: 1.0023 - acc: 0.4503\n",
            "Epoch 134/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 135/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 136/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0016 - acc: 0.4503\n",
            "Epoch 137/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 138/150\n",
            "171/171 [==============================] - 0s 112us/step - loss: 1.0024 - acc: 0.4503\n",
            "Epoch 139/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 140/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 141/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 1.0023 - acc: 0.4503\n",
            "Epoch 142/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 143/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 144/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0023 - acc: 0.4503\n",
            "Epoch 145/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 1.0021 - acc: 0.4503\n",
            "Epoch 146/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0018 - acc: 0.4503\n",
            "Epoch 147/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 1.0020 - acc: 0.4503\n",
            "Epoch 148/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 1.0033 - acc: 0.4503\n",
            "Epoch 149/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 1.0028 - acc: 0.4503\n",
            "Epoch 150/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0024 - acc: 0.4503\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbb6d4de860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 173
        }
      ]
    },
    {
      "metadata": {
        "id": "MQHWwS_Hmln5",
        "colab_type": "code",
        "outputId": "6cc8e86f-3f8a-4f68-f2c4-7e846191e863",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        }
      },
      "cell_type": "code",
      "source": [
        "scores6 = model6.evaluate(pre_test, diff_test, batch_size=16)\n",
        "print(\"\\n%s: %.2f%%\" % (model6.metrics_names[1], scores6[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "74/74 [==============================] - 0s 3ms/step\n",
            "\n",
            "acc: 37.84%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "TV2Pd7uqmuww",
        "colab_type": "code",
        "outputId": "963ca4ab-a347-4690-ee30-36be597896c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5279
        }
      },
      "cell_type": "code",
      "source": [
        "inputs7 = Input(shape=(6,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x7 = Dense(64, activation='relu')(inputs7)\n",
        "x7 = Dense(64, activation='relu')(x7)\n",
        "predictions7 = Dense(3, activation='softmax')(x7)\n",
        "\n",
        "# This creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model7 = Model(inputs=inputs7, outputs=predictions7)\n",
        "model7.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model7.fit(pre_actions_train, diff_train, batch_size=16, epochs=150)  # starts training\n",
        "scores7 = model7.evaluate(pre_actions_test, diff_test, batch_size=16)\n",
        "print(\"\\n%s: %.2f%%\" % (model7.metrics_names[1], scores7[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "171/171 [==============================] - 1s 4ms/step - loss: 1.1066 - acc: 0.2749\n",
            "Epoch 2/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 1.0332 - acc: 0.4327\n",
            "Epoch 3/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 1.0100 - acc: 0.4620\n",
            "Epoch 4/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0075 - acc: 0.4620\n",
            "Epoch 5/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0013 - acc: 0.4620\n",
            "Epoch 6/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9981 - acc: 0.4620\n",
            "Epoch 7/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9947 - acc: 0.4737\n",
            "Epoch 8/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9926 - acc: 0.4795\n",
            "Epoch 9/150\n",
            "171/171 [==============================] - 0s 116us/step - loss: 0.9918 - acc: 0.4795\n",
            "Epoch 10/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9889 - acc: 0.4795\n",
            "Epoch 11/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.9879 - acc: 0.4561\n",
            "Epoch 12/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9844 - acc: 0.4912\n",
            "Epoch 13/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9837 - acc: 0.4620\n",
            "Epoch 14/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.9831 - acc: 0.4327\n",
            "Epoch 15/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.9822 - acc: 0.4620\n",
            "Epoch 16/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9810 - acc: 0.4737\n",
            "Epoch 17/150\n",
            "171/171 [==============================] - 0s 111us/step - loss: 0.9790 - acc: 0.4971\n",
            "Epoch 18/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9782 - acc: 0.4971\n",
            "Epoch 19/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9765 - acc: 0.4971\n",
            "Epoch 20/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.9780 - acc: 0.4678\n",
            "Epoch 21/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.9762 - acc: 0.4561\n",
            "Epoch 22/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.9739 - acc: 0.4971\n",
            "Epoch 23/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.9723 - acc: 0.4971\n",
            "Epoch 24/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9732 - acc: 0.4971\n",
            "Epoch 25/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 0.9702 - acc: 0.4971\n",
            "Epoch 26/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9713 - acc: 0.4971\n",
            "Epoch 27/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9691 - acc: 0.4795\n",
            "Epoch 28/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.9687 - acc: 0.4912\n",
            "Epoch 29/150\n",
            "171/171 [==============================] - 0s 114us/step - loss: 0.9687 - acc: 0.4971\n",
            "Epoch 30/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 0.9681 - acc: 0.4971\n",
            "Epoch 31/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9672 - acc: 0.4912\n",
            "Epoch 32/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9666 - acc: 0.4971\n",
            "Epoch 33/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9679 - acc: 0.4971\n",
            "Epoch 34/150\n",
            "171/171 [==============================] - 0s 117us/step - loss: 0.9677 - acc: 0.4971\n",
            "Epoch 35/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.9657 - acc: 0.4971\n",
            "Epoch 36/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.9647 - acc: 0.4971\n",
            "Epoch 37/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.9647 - acc: 0.4971\n",
            "Epoch 38/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9642 - acc: 0.5088\n",
            "Epoch 39/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9640 - acc: 0.4971\n",
            "Epoch 40/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.9650 - acc: 0.4795\n",
            "Epoch 41/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.9629 - acc: 0.4971\n",
            "Epoch 42/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9636 - acc: 0.4912\n",
            "Epoch 43/150\n",
            "171/171 [==============================] - 0s 115us/step - loss: 0.9638 - acc: 0.4971\n",
            "Epoch 44/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.9633 - acc: 0.4678\n",
            "Epoch 45/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9626 - acc: 0.4912\n",
            "Epoch 46/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9624 - acc: 0.4795\n",
            "Epoch 47/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.9626 - acc: 0.4971\n",
            "Epoch 48/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9623 - acc: 0.4912\n",
            "Epoch 49/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9636 - acc: 0.4912\n",
            "Epoch 50/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.9619 - acc: 0.4971\n",
            "Epoch 51/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.9607 - acc: 0.4854\n",
            "Epoch 52/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.9616 - acc: 0.4971\n",
            "Epoch 53/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.9614 - acc: 0.4971\n",
            "Epoch 54/150\n",
            "171/171 [==============================] - 0s 121us/step - loss: 0.9611 - acc: 0.4795\n",
            "Epoch 55/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9618 - acc: 0.4854\n",
            "Epoch 56/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9610 - acc: 0.4971\n",
            "Epoch 57/150\n",
            "171/171 [==============================] - 0s 134us/step - loss: 0.9617 - acc: 0.4678\n",
            "Epoch 58/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9611 - acc: 0.4971\n",
            "Epoch 59/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9613 - acc: 0.4971\n",
            "Epoch 60/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9612 - acc: 0.4912\n",
            "Epoch 61/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.9612 - acc: 0.4971\n",
            "Epoch 62/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9611 - acc: 0.4971\n",
            "Epoch 63/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9613 - acc: 0.4737\n",
            "Epoch 64/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 0.9605 - acc: 0.4971\n",
            "Epoch 65/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.9610 - acc: 0.4971\n",
            "Epoch 66/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9597 - acc: 0.4971\n",
            "Epoch 67/150\n",
            "171/171 [==============================] - 0s 109us/step - loss: 0.9613 - acc: 0.4912\n",
            "Epoch 68/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9613 - acc: 0.4971\n",
            "Epoch 69/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.9629 - acc: 0.5029\n",
            "Epoch 70/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9607 - acc: 0.4971\n",
            "Epoch 71/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.9596 - acc: 0.4912\n",
            "Epoch 72/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 0.9610 - acc: 0.4795\n",
            "Epoch 73/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9599 - acc: 0.4971\n",
            "Epoch 74/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9596 - acc: 0.4912\n",
            "Epoch 75/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9603 - acc: 0.4971\n",
            "Epoch 76/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.9613 - acc: 0.4971\n",
            "Epoch 77/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9597 - acc: 0.4971\n",
            "Epoch 78/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.9605 - acc: 0.4912\n",
            "Epoch 79/150\n",
            "171/171 [==============================] - 0s 109us/step - loss: 0.9614 - acc: 0.4971\n",
            "Epoch 80/150\n",
            "171/171 [==============================] - 0s 147us/step - loss: 0.9613 - acc: 0.4971\n",
            "Epoch 81/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9612 - acc: 0.4912\n",
            "Epoch 82/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 0.9597 - acc: 0.4971\n",
            "Epoch 83/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.9596 - acc: 0.4971\n",
            "Epoch 84/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9590 - acc: 0.4971\n",
            "Epoch 85/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.9595 - acc: 0.4854\n",
            "Epoch 86/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9598 - acc: 0.4971\n",
            "Epoch 87/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9594 - acc: 0.4971\n",
            "Epoch 88/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9593 - acc: 0.4971\n",
            "Epoch 89/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.9591 - acc: 0.4971\n",
            "Epoch 90/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9593 - acc: 0.4795\n",
            "Epoch 91/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9597 - acc: 0.4912\n",
            "Epoch 92/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 0.9596 - acc: 0.4971\n",
            "Epoch 93/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9596 - acc: 0.4971\n",
            "Epoch 94/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9588 - acc: 0.4971\n",
            "Epoch 95/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9597 - acc: 0.4912\n",
            "Epoch 96/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 0.9592 - acc: 0.4971\n",
            "Epoch 97/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.9594 - acc: 0.4971\n",
            "Epoch 98/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9591 - acc: 0.4971\n",
            "Epoch 99/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9599 - acc: 0.4912\n",
            "Epoch 100/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9590 - acc: 0.4971\n",
            "Epoch 101/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9593 - acc: 0.4971\n",
            "Epoch 102/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 0.9597 - acc: 0.4795\n",
            "Epoch 103/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9603 - acc: 0.4912\n",
            "Epoch 104/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9624 - acc: 0.4678\n",
            "Epoch 105/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9609 - acc: 0.4971\n",
            "Epoch 106/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.9594 - acc: 0.4971\n",
            "Epoch 107/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.9604 - acc: 0.4912\n",
            "Epoch 108/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 0.9603 - acc: 0.4971\n",
            "Epoch 109/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9612 - acc: 0.4912\n",
            "Epoch 110/150\n",
            "171/171 [==============================] - 0s 116us/step - loss: 0.9591 - acc: 0.4971\n",
            "Epoch 111/150\n",
            "171/171 [==============================] - 0s 111us/step - loss: 0.9584 - acc: 0.4971\n",
            "Epoch 112/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9587 - acc: 0.4912\n",
            "Epoch 113/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9602 - acc: 0.5029\n",
            "Epoch 114/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.9594 - acc: 0.4912\n",
            "Epoch 115/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9612 - acc: 0.5088\n",
            "Epoch 116/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.9591 - acc: 0.4971\n",
            "Epoch 117/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9624 - acc: 0.4971\n",
            "Epoch 118/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.9589 - acc: 0.4795\n",
            "Epoch 119/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9588 - acc: 0.4912\n",
            "Epoch 120/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 0.9586 - acc: 0.4971\n",
            "Epoch 121/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9591 - acc: 0.4912\n",
            "Epoch 122/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 0.9594 - acc: 0.4795\n",
            "Epoch 123/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.9593 - acc: 0.4971\n",
            "Epoch 124/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.9589 - acc: 0.4971\n",
            "Epoch 125/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9596 - acc: 0.4971\n",
            "Epoch 126/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.9584 - acc: 0.4971\n",
            "Epoch 127/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9587 - acc: 0.4971\n",
            "Epoch 128/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9596 - acc: 0.4795\n",
            "Epoch 129/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.9593 - acc: 0.4795\n",
            "Epoch 130/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 0.9593 - acc: 0.4971\n",
            "Epoch 131/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9600 - acc: 0.4795\n",
            "Epoch 132/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.9600 - acc: 0.4795\n",
            "Epoch 133/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9586 - acc: 0.4971\n",
            "Epoch 134/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9594 - acc: 0.4971\n",
            "Epoch 135/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 0.9596 - acc: 0.4971\n",
            "Epoch 136/150\n",
            "171/171 [==============================] - 0s 82us/step - loss: 0.9585 - acc: 0.4971\n",
            "Epoch 137/150\n",
            "171/171 [==============================] - 0s 86us/step - loss: 0.9598 - acc: 0.4912\n",
            "Epoch 138/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9582 - acc: 0.4795\n",
            "Epoch 139/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9610 - acc: 0.4971\n",
            "Epoch 140/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 0.9593 - acc: 0.4971\n",
            "Epoch 141/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9587 - acc: 0.4971\n",
            "Epoch 142/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 0.9579 - acc: 0.4737\n",
            "Epoch 143/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.9585 - acc: 0.4971\n",
            "Epoch 144/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.9593 - acc: 0.4971\n",
            "Epoch 145/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9589 - acc: 0.4971\n",
            "Epoch 146/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.9596 - acc: 0.4620\n",
            "Epoch 147/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 0.9608 - acc: 0.4971\n",
            "Epoch 148/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.9585 - acc: 0.4971\n",
            "Epoch 149/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9589 - acc: 0.4854\n",
            "Epoch 150/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9591 - acc: 0.4854\n",
            "74/74 [==============================] - 0s 3ms/step\n",
            "\n",
            "acc: 32.43%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "1NGo41ksnLHB",
        "colab_type": "code",
        "outputId": "737e3f6c-71e5-422b-cfb7-1eb8be509452",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5279
        }
      },
      "cell_type": "code",
      "source": [
        "inputs8 = Input(shape=(3,))\n",
        "\n",
        "# a layer instance is callable on a tensor, and returns a tensor\n",
        "x8 = Dense(64, activation='relu')(inputs8)\n",
        "x8 = Dense(64, activation='relu')(x8)\n",
        "predictions8 = Dense(3, activation='softmax')(x8)\n",
        "\n",
        "# This creates a model that includes\n",
        "# the Input layer and three Dense layers\n",
        "model8 = Model(inputs=inputs8, outputs=predictions8)\n",
        "model8.compile(optimizer='adam',\n",
        "              loss='categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "model8.fit(actions_train, diff_train, batch_size=16, epochs=150)  # starts training\n",
        "scores8 = model8.evaluate(actions_test, diff_test, batch_size=16)\n",
        "print(\"\\n%s: %.2f%%\" % (model8.metrics_names[1], scores8[1]*100))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/150\n",
            "171/171 [==============================] - 1s 4ms/step - loss: 1.0915 - acc: 0.3801\n",
            "Epoch 2/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0467 - acc: 0.4269\n",
            "Epoch 3/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 1.0199 - acc: 0.4444\n",
            "Epoch 4/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 1.0112 - acc: 0.3918\n",
            "Epoch 5/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 1.0057 - acc: 0.4386\n",
            "Epoch 6/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 1.0045 - acc: 0.4386\n",
            "Epoch 7/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 1.0035 - acc: 0.4386\n",
            "Epoch 8/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 1.0017 - acc: 0.4386\n",
            "Epoch 9/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 1.0031 - acc: 0.3801\n",
            "Epoch 10/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 1.0004 - acc: 0.4444\n",
            "Epoch 11/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.9994 - acc: 0.4386\n",
            "Epoch 12/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.9965 - acc: 0.4444\n",
            "Epoch 13/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9967 - acc: 0.4386\n",
            "Epoch 14/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.9959 - acc: 0.4386\n",
            "Epoch 15/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9980 - acc: 0.3275\n",
            "Epoch 16/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9969 - acc: 0.3977\n",
            "Epoch 17/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.9955 - acc: 0.4386\n",
            "Epoch 18/150\n",
            "171/171 [==============================] - 0s 87us/step - loss: 0.9951 - acc: 0.3860\n",
            "Epoch 19/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.9955 - acc: 0.4152\n",
            "Epoch 20/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.9955 - acc: 0.4211\n",
            "Epoch 21/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9960 - acc: 0.4444\n",
            "Epoch 22/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.9939 - acc: 0.4444\n",
            "Epoch 23/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.9938 - acc: 0.4094\n",
            "Epoch 24/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9943 - acc: 0.4386\n",
            "Epoch 25/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9965 - acc: 0.3392\n",
            "Epoch 26/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.9939 - acc: 0.4152\n",
            "Epoch 27/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9934 - acc: 0.4444\n",
            "Epoch 28/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9945 - acc: 0.3860\n",
            "Epoch 29/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9950 - acc: 0.3860\n",
            "Epoch 30/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.9948 - acc: 0.4152\n",
            "Epoch 31/150\n",
            "171/171 [==============================] - 0s 140us/step - loss: 0.9936 - acc: 0.4444\n",
            "Epoch 32/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.9931 - acc: 0.3801\n",
            "Epoch 33/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9936 - acc: 0.4444\n",
            "Epoch 34/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.9943 - acc: 0.4444\n",
            "Epoch 35/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.9934 - acc: 0.4444\n",
            "Epoch 36/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.9941 - acc: 0.3977\n",
            "Epoch 37/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9928 - acc: 0.4152\n",
            "Epoch 38/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 0.9937 - acc: 0.4444\n",
            "Epoch 39/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9946 - acc: 0.4035\n",
            "Epoch 40/150\n",
            "171/171 [==============================] - 0s 111us/step - loss: 0.9935 - acc: 0.4094\n",
            "Epoch 41/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9934 - acc: 0.4152\n",
            "Epoch 42/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.9936 - acc: 0.4094\n",
            "Epoch 43/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9929 - acc: 0.4211\n",
            "Epoch 44/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9925 - acc: 0.4444\n",
            "Epoch 45/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9940 - acc: 0.4094\n",
            "Epoch 46/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9935 - acc: 0.4444\n",
            "Epoch 47/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9941 - acc: 0.4444\n",
            "Epoch 48/150\n",
            "171/171 [==============================] - 0s 114us/step - loss: 0.9929 - acc: 0.4152\n",
            "Epoch 49/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.9931 - acc: 0.4152\n",
            "Epoch 50/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.9937 - acc: 0.3801\n",
            "Epoch 51/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.9939 - acc: 0.4094\n",
            "Epoch 52/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9928 - acc: 0.4269\n",
            "Epoch 53/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9926 - acc: 0.4327\n",
            "Epoch 54/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9965 - acc: 0.4444\n",
            "Epoch 55/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 0.9951 - acc: 0.3977\n",
            "Epoch 56/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.9923 - acc: 0.4444\n",
            "Epoch 57/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.9929 - acc: 0.4444\n",
            "Epoch 58/150\n",
            "171/171 [==============================] - 0s 111us/step - loss: 0.9928 - acc: 0.4444\n",
            "Epoch 59/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.9942 - acc: 0.4444\n",
            "Epoch 60/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.9941 - acc: 0.4094\n",
            "Epoch 61/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.9922 - acc: 0.4444\n",
            "Epoch 62/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9929 - acc: 0.4269\n",
            "Epoch 63/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9925 - acc: 0.4444\n",
            "Epoch 64/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.9928 - acc: 0.4444\n",
            "Epoch 65/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9928 - acc: 0.4269\n",
            "Epoch 66/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.9932 - acc: 0.4035\n",
            "Epoch 67/150\n",
            "171/171 [==============================] - 0s 114us/step - loss: 0.9928 - acc: 0.4444\n",
            "Epoch 68/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.9937 - acc: 0.4386\n",
            "Epoch 69/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.9931 - acc: 0.4269\n",
            "Epoch 70/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9929 - acc: 0.4211\n",
            "Epoch 71/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9939 - acc: 0.4444\n",
            "Epoch 72/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9927 - acc: 0.4152\n",
            "Epoch 73/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.9944 - acc: 0.4211\n",
            "Epoch 74/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 0.9940 - acc: 0.4035\n",
            "Epoch 75/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.9937 - acc: 0.4386\n",
            "Epoch 76/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.9923 - acc: 0.4327\n",
            "Epoch 77/150\n",
            "171/171 [==============================] - 0s 109us/step - loss: 0.9927 - acc: 0.4211\n",
            "Epoch 78/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 0.9938 - acc: 0.4444\n",
            "Epoch 79/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 0.9948 - acc: 0.4327\n",
            "Epoch 80/150\n",
            "171/171 [==============================] - 0s 82us/step - loss: 0.9927 - acc: 0.4444\n",
            "Epoch 81/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.9922 - acc: 0.4444\n",
            "Epoch 82/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 0.9922 - acc: 0.4444\n",
            "Epoch 83/150\n",
            "171/171 [==============================] - 0s 125us/step - loss: 0.9921 - acc: 0.4269\n",
            "Epoch 84/150\n",
            "171/171 [==============================] - 0s 125us/step - loss: 0.9920 - acc: 0.4152\n",
            "Epoch 85/150\n",
            "171/171 [==============================] - 0s 119us/step - loss: 0.9927 - acc: 0.4035\n",
            "Epoch 86/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9932 - acc: 0.4211\n",
            "Epoch 87/150\n",
            "171/171 [==============================] - 0s 109us/step - loss: 0.9938 - acc: 0.4269\n",
            "Epoch 88/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 0.9928 - acc: 0.4444\n",
            "Epoch 89/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.9923 - acc: 0.4444\n",
            "Epoch 90/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9938 - acc: 0.4094\n",
            "Epoch 91/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.9918 - acc: 0.4386\n",
            "Epoch 92/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.9934 - acc: 0.4444\n",
            "Epoch 93/150\n",
            "171/171 [==============================] - 0s 101us/step - loss: 0.9925 - acc: 0.4327\n",
            "Epoch 94/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.9931 - acc: 0.3860\n",
            "Epoch 95/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.9925 - acc: 0.4444\n",
            "Epoch 96/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9923 - acc: 0.3977\n",
            "Epoch 97/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9936 - acc: 0.4035\n",
            "Epoch 98/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9923 - acc: 0.4211\n",
            "Epoch 99/150\n",
            "171/171 [==============================] - 0s 112us/step - loss: 0.9928 - acc: 0.4444\n",
            "Epoch 100/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.9920 - acc: 0.4503\n",
            "Epoch 101/150\n",
            "171/171 [==============================] - 0s 107us/step - loss: 0.9925 - acc: 0.4444\n",
            "Epoch 102/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9928 - acc: 0.4444\n",
            "Epoch 103/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.9928 - acc: 0.4094\n",
            "Epoch 104/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 0.9932 - acc: 0.4444\n",
            "Epoch 105/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9923 - acc: 0.4444\n",
            "Epoch 106/150\n",
            "171/171 [==============================] - 0s 105us/step - loss: 0.9936 - acc: 0.4269\n",
            "Epoch 107/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 0.9939 - acc: 0.4620\n",
            "Epoch 108/150\n",
            "171/171 [==============================] - 0s 90us/step - loss: 0.9927 - acc: 0.4444\n",
            "Epoch 109/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9936 - acc: 0.4444\n",
            "Epoch 110/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.9929 - acc: 0.4094\n",
            "Epoch 111/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9936 - acc: 0.4444\n",
            "Epoch 112/150\n",
            "171/171 [==============================] - 0s 97us/step - loss: 0.9926 - acc: 0.4444\n",
            "Epoch 113/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.9934 - acc: 0.4094\n",
            "Epoch 114/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9928 - acc: 0.4211\n",
            "Epoch 115/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9929 - acc: 0.4035\n",
            "Epoch 116/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9924 - acc: 0.4327\n",
            "Epoch 117/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9931 - acc: 0.4444\n",
            "Epoch 118/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9941 - acc: 0.4269\n",
            "Epoch 119/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9926 - acc: 0.4561\n",
            "Epoch 120/150\n",
            "171/171 [==============================] - 0s 103us/step - loss: 0.9924 - acc: 0.4444\n",
            "Epoch 121/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.9925 - acc: 0.4444\n",
            "Epoch 122/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9938 - acc: 0.4386\n",
            "Epoch 123/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9935 - acc: 0.4211\n",
            "Epoch 124/150\n",
            "171/171 [==============================] - 0s 104us/step - loss: 0.9928 - acc: 0.4444\n",
            "Epoch 125/150\n",
            "171/171 [==============================] - 0s 91us/step - loss: 0.9929 - acc: 0.4444\n",
            "Epoch 126/150\n",
            "171/171 [==============================] - 0s 94us/step - loss: 0.9922 - acc: 0.4444\n",
            "Epoch 127/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.9923 - acc: 0.4444\n",
            "Epoch 128/150\n",
            "171/171 [==============================] - 0s 98us/step - loss: 0.9927 - acc: 0.4444\n",
            "Epoch 129/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9929 - acc: 0.4444\n",
            "Epoch 130/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9929 - acc: 0.4152\n",
            "Epoch 131/150\n",
            "171/171 [==============================] - 0s 110us/step - loss: 0.9928 - acc: 0.3860\n",
            "Epoch 132/150\n",
            "171/171 [==============================] - 0s 85us/step - loss: 0.9923 - acc: 0.4444\n",
            "Epoch 133/150\n",
            "171/171 [==============================] - 0s 102us/step - loss: 0.9929 - acc: 0.3860\n",
            "Epoch 134/150\n",
            "171/171 [==============================] - 0s 95us/step - loss: 0.9934 - acc: 0.4094\n",
            "Epoch 135/150\n",
            "171/171 [==============================] - 0s 99us/step - loss: 0.9922 - acc: 0.4444\n",
            "Epoch 136/150\n",
            "171/171 [==============================] - 0s 108us/step - loss: 0.9927 - acc: 0.4327\n",
            "Epoch 137/150\n",
            "171/171 [==============================] - 0s 112us/step - loss: 0.9927 - acc: 0.4386\n",
            "Epoch 138/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.9931 - acc: 0.3860\n",
            "Epoch 139/150\n",
            "171/171 [==============================] - 0s 124us/step - loss: 0.9932 - acc: 0.3977\n",
            "Epoch 140/150\n",
            "171/171 [==============================] - 0s 111us/step - loss: 0.9944 - acc: 0.4211\n",
            "Epoch 141/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.9923 - acc: 0.4444\n",
            "Epoch 142/150\n",
            "171/171 [==============================] - 0s 100us/step - loss: 0.9943 - acc: 0.4035\n",
            "Epoch 143/150\n",
            "171/171 [==============================] - 0s 114us/step - loss: 0.9921 - acc: 0.4444\n",
            "Epoch 144/150\n",
            "171/171 [==============================] - 0s 93us/step - loss: 0.9927 - acc: 0.4327\n",
            "Epoch 145/150\n",
            "171/171 [==============================] - 0s 92us/step - loss: 0.9927 - acc: 0.4444\n",
            "Epoch 146/150\n",
            "171/171 [==============================] - 0s 96us/step - loss: 0.9931 - acc: 0.4094\n",
            "Epoch 147/150\n",
            "171/171 [==============================] - 0s 88us/step - loss: 0.9931 - acc: 0.4211\n",
            "Epoch 148/150\n",
            "171/171 [==============================] - 0s 89us/step - loss: 0.9922 - acc: 0.4386\n",
            "Epoch 149/150\n",
            "171/171 [==============================] - 0s 106us/step - loss: 0.9929 - acc: 0.4444\n",
            "Epoch 150/150\n",
            "171/171 [==============================] - 0s 118us/step - loss: 0.9923 - acc: 0.4444\n",
            "74/74 [==============================] - 0s 3ms/step\n",
            "\n",
            "acc: 35.14%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PdhE-33d7wYs",
        "colab_type": "code",
        "outputId": "995c8366-cbe0-44e5-fd26-c1c0ff36bc2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "cvscores = []\n",
        "for train, test in kfold.split(matrix_pre_only, matrix_output):\n",
        "  # create model\n",
        "  pre_train, pre_test = matrix_pre_only[train], matrix_pre_only[test]\n",
        "  post_train, post_test = matrix_output[train], matrix_output[test]\n",
        "  inputs3 = Input(shape=(3,))\n",
        "  # a layer instance is callable on a tensor, and returns a tensor\n",
        "  x3 = Dense(64, activation='relu')(inputs3)\n",
        "  x3 = Dense(64, activation='relu')(x3)\n",
        "  predictions3 = Dense(3, activation='softmax')(x3)\n",
        "  # This creates a model that includes\n",
        "  # the Input layer and three Dense layers\n",
        "  model3 = Model(inputs=inputs3, outputs=predictions3)\n",
        "  model3.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  model3.fit(pre_train, post_train, batch_size=10, verbose=0, epochs=150)\n",
        "  scores3 = model3.evaluate(pre_test, post_test, verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model3.metrics_names[1], scores3[1]*100))\n",
        "  cvscores.append(scores3[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 69.39%\n",
            "acc: 69.39%\n",
            "acc: 75.51%\n",
            "acc: 69.39%\n",
            "acc: 71.43%\n",
            "71.02% (+/- 2.38%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GVSpkmZT8hLB",
        "colab_type": "code",
        "outputId": "40087381-b532-4907-cb24-edd1ac0343e6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "cvscores = []\n",
        "for train, test in kfold.split(matrix, matrix_output):\n",
        "  # create model\n",
        "  pre_actions_train, pre_actions_test = matrix[train], matrix[test]\n",
        "  post_train, post_test = matrix_output[train], matrix_output[test]\n",
        "  inputs4 = Input(shape=(6,))\n",
        "  # a layer instance is callable on a tensor, and returns a tensor\n",
        "  x4 = Dense(64, activation='relu')(inputs4)\n",
        "  x4 = Dense(64, activation='relu')(x4)\n",
        "  predictions4 = Dense(3, activation='softmax')(x4)\n",
        "  # This creates a model that includes\n",
        "  # the Input layer and three Dense layers\n",
        "  model4 = Model(inputs=inputs4, outputs=predictions4)\n",
        "  model4.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  model4.fit(pre_actions_train, post_train, batch_size=10, verbose=0, epochs=150)\n",
        "  scores4 = model4.evaluate(pre_actions_test, post_test, verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model4.metrics_names[1], scores4[1]*100))\n",
        "  cvscores.append(scores4[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "acc: 69.39%\n",
            "acc: 69.39%\n",
            "acc: 75.51%\n",
            "acc: 65.31%\n",
            "acc: 71.43%\n",
            "70.20% (+/- 3.32%)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dAygMXwTqrP0",
        "colab_type": "code",
        "outputId": "b60dda85-98cf-430d-d923-a57f81252d5d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "matrix.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(245, 6)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 184
        }
      ]
    },
    {
      "metadata": {
        "id": "weNrSJ4Hq0Oh",
        "colab_type": "code",
        "outputId": "b03269bf-b400-4ee4-d568-bc973927ea64",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "matrix_diff.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(245, 3)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 185
        }
      ]
    },
    {
      "metadata": {
        "id": "cTIGQDplnjUU",
        "colab_type": "code",
        "outputId": "2ae05764-af0e-4a35-9b39-a07faad69f96",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 619
        }
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "cvscores = []\n",
        "for train, test in kfold.split(matrix, matrix_output):\n",
        "  # create model\n",
        "  pre_actions_train, pre_actions_test = matrix[train], matrix[test]\n",
        "  post_train, post_test = matrix_output[train], matrix_output[test]\n",
        "  inputs4 = Input(shape=(6,))\n",
        "  # a layer instance is callable on a tensor, and returns a tensor\n",
        "  x4 = Dense(64, activation='relu')(inputs4)\n",
        "  x4 = Dense(64, activation='relu')(x4)\n",
        "  predictions4 = Dense(3, activation='softmax')(x4)\n",
        "  # This creates a model that includes\n",
        "  # the Input layer and three Dense layers\n",
        "  model4 = Model(inputs=inputs4, outputs=predictions4)\n",
        "  model4.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  model4.fit(pre_actions_train, diff_train, batch_size=10, verbose=0, epochs=150)\n",
        "  scores4 = model4.evaluate(pre_actions_test, diff_test, verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model4.metrics_names[1], scores4[1]*100))\n",
        "  cvscores.append(scores4[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-179-1200b707a9ac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'categorical_crossentropy'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m                 metrics=['accuracy'])\n\u001b[0;32m---> 19\u001b[0;31m   \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_actions_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m150\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m   \u001b[0mscores4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpre_actions_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiff_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%s: %.2f%%\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmodel4\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics_names\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscores4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 952\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m    953\u001b[0m         \u001b[0;31m# Prepare validation data.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    802\u001b[0m             ]\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# Check that all arrays have the same length.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 804\u001b[0;31m             \u001b[0mcheck_array_length_consistency\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    805\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_graph_network\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    806\u001b[0m                 \u001b[0;31m# Additional checks to avoid users mistakenly\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mcheck_array_length_consistency\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    235\u001b[0m                          \u001b[0;34m'the same number of samples as target arrays. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m                          \u001b[0;34m'Found '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' input samples '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 237\u001b[0;31m                          'and ' + str(list(set_y)[0]) + ' target samples.')\n\u001b[0m\u001b[1;32m    238\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_w\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m         raise ValueError('All sample_weight arrays should have '\n",
            "\u001b[0;31mValueError\u001b[0m: Input arrays should have the same number of samples as target arrays. Found 196 input samples and 171 target samples."
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "et_UQYe49SFe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import StratifiedKFold, KFold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=1)\n",
        "cvscores = []\n",
        "for train, test in kfold.split(matrix_actions, matrix_output):\n",
        "  # create model\n",
        "  actions_train, actions_test = matrix_actions[train], matrix_actions[test]\n",
        "  post_train, post_test = matrix_output[train], matrix_output[test]\n",
        "  inputs5 = Input(shape=(3,))\n",
        "  # a layer instance is callable on a tensor, and returns a tensor\n",
        "  x5 = Dense(64, activation='relu')(inputs5)\n",
        "  x5 = Dense(64, activation='relu')(x5)\n",
        "  predictions5 = Dense(3, activation='softmax')(x5)\n",
        "  # This creates a model that includes\n",
        "  # the Input layer and three Dense layers\n",
        "  model5 = Model(inputs=inputs5, outputs=predictions5)\n",
        "  model5.compile(optimizer='adam',\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'])\n",
        "  model5.fit(actions_train, post_train, batch_size=10, verbose=0, epochs=150)\n",
        "  scores5 = model5.evaluate(actions_test, post_test, verbose=0)\n",
        "  print(\"%s: %.2f%%\" % (model5.metrics_names[1], scores5[1]*100))\n",
        "  cvscores.append(scores5[1] * 100)\n",
        "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tA2QMG_kFbOD",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ">pre predicts post | pre & action predicts post | action predicts post\n",
        ">--- | --- | ---\n",
        ">acc: 77.14% | acc: 77.14% | acc: 60.00%\n",
        ">acc: 67.65% | acc: 67.65% | acc: 50.00%\n",
        ">acc: 70.59% | acc: 70.59% | acc: 47.06%\n",
        ">acc: 76.47% | acc: 73.53% | acc: 50.00%\n",
        ">acc: 76.47% | acc: 76.47% | acc: 47.06%\n",
        ">73.66% (+/- 3.83%) | 73.08% (+/- 3.58%) | 50.82% (+/- 4.77%)"
      ]
    },
    {
      "metadata": {
        "id": "zchJoFVB36bU",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "print('pre_actions_train: %s, pre_actions_test: %s' % (matrix[train], matrix[test]))\n",
        "print('post_train: %s, post_test: %s' % (matrix_output[train], matrix[test]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "y3MKnNDiE79c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''tsne_model = TSNE(perplexity=40, n_components=2, n_iter=4000, random_state=23) #use n_iter = 4000 for skills, 1000 for assistments\n",
        "new_values = tsne_model.fit_transform(inputs_array)\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WHkcaxAoxyCn",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Clustering"
      ]
    },
    {
      "metadata": {
        "id": "gzPqzxX8xiYs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.manifold import TSNE\n",
        "import matplotlib.pyplot as plt\n",
        "new_grouped = new.groupby(['userId', 'gameLevel'])['actionName'].count()\n",
        "new_grouped = new_grouped.to_frame()\n",
        "new_grouped.columns = ['count']\n",
        "new_grouped = new_grouped.reset_index()\n",
        "\n",
        "questions = ['1.05b', '2.01c', '3.02b', '4.07b', 'T1.02a', 'T1.02b', 'T1.04', 'T1.07a', 'T3.01a', 'T4.01a', 'T4.01b', 'T4.02']\n",
        "for index, row in new_grouped.iterrows():\n",
        "    if row['gameLevel'] not in questions:\n",
        "        new_grouped.drop(index, inplace=True)\n",
        "participants = new_grouped['userId'].tolist()\n",
        "output = []\n",
        "for x in participants:\n",
        "    if x not in output:\n",
        "        output.append(x)\n",
        "participants = output\n",
        "tsne_input = new_grouped.set_index(['gameLevel', 'userId']).unstack(level=0)\n",
        "tsne_input = tsne_input.fillna(0)\n",
        "colors = []\n",
        "for i in range(len(pre_post_mean)):\n",
        "  if pre_post_mean.post_groupings[i] == 'low':\n",
        "    colors.append(\"red\")\n",
        "  elif pre_post_mean.post_groupings[i] == 'medium':\n",
        "    colors.append(\"yellow\")\n",
        "  elif pre_post_mean.post_groupings[i] == 'high':\n",
        "    colors.append(\"blue\")\n",
        "pre_post_mean['color'] = colors\n",
        "df = pre_post_mean[['userId', 'color']]\n",
        "df.userId = df.userId.astype(int)\n",
        "colors = df.set_index('userId')['color'].to_dict()\n",
        "def tsne_plot(inputs):\n",
        "    \"Creates and TSNE model and plots it\" \n",
        "    tsne_model = TSNE(perplexity=40, n_components=2, n_iter=300) #use n_iter = 4000 for skills, 1000 for assistments\n",
        "    new_values = tsne_model.fit_transform(inputs)\n",
        "\n",
        "    x = []\n",
        "    y = []\n",
        "    labels = participants\n",
        "    for value in new_values:\n",
        "        x.append(value[0])\n",
        "        y.append(value[1])\n",
        "        \n",
        "        \n",
        "    plt.figure(figsize=(16, 16)) \n",
        "    \n",
        "    for i in range(len(x)):  \n",
        "        if participants[i] in colors:\n",
        "          plt.scatter(x[i],y[i], c = colors[participants[i]])\n",
        "        else:\n",
        "          plt.scatter(x[i],y[i], c = \"white\")\n",
        "        \n",
        "     \n",
        "    plt.show()\n",
        "    \n",
        "tsne_plot(tsne_input)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-T4gP7AcKcud",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#new_actions = new[['userId', 'gameLevel', 'actionName']].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HttyTWvwLsFB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#new_actions.groupby(['userId', 'gameLevel'])['actionName'].count()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "U10pILfhIiTK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#new_groupedf = pd.DataFrame(new_grouped)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eICqNv4-NATx",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''new_groupedf.groupby(['userId', 'gameLevel']).apply(lambda x: pd.Series(\n",
        "                               {\n",
        "                                **{''+str(i+1): t for i,t in enumerate(x.Code)},\n",
        "                                **{'Score'+str(i+1): t for i,t in enumerate(x.Score)}\n",
        "                               }\n",
        "                             )).reset_index()'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wxIbhWivH-gy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#new_actions.pivot(index='userId', columns='gameLevel', values='actionName')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MYnkasErOBWG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#new_list = new['sentences'].tolist()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "HkX113RdUJqe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''import nltk\n",
        "from nltk import ngrams\n",
        "from nltk.collocations import *\n",
        "n = 4\n",
        "#fourgrams = ngrams(new_list, n)'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iKqfRQTuwIhr",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#mygrams = ngrams(mysetence2, n=4)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8ZoszMefwTlz",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#for grams in mygrams:\n",
        " # print(grams)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "c1WyQRVtZj10",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#compute frequency distribution for all the bigrams in the text\n",
        "'''fdist = nltk.FreqDist(fourgrams)\n",
        "sequences = {}\n",
        "for k,v in fdist.items():\n",
        "  print(k,v)\n",
        "  if int(v) > 2:\n",
        "    #print(v)\n",
        "    sequences[k] = v'''\n",
        "#print(sequences)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "i1KEgPGkhe6I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#seq_df = pd.DataFrame.from_dict(sequences, orient='index')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "09Kcd_RNh-ie",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#seq_df.to_csv(\"sequences.csv\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wx1gND2_cwT4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "'''sequences = {}\n",
        "for k,v in fdist.items():\n",
        "  print(k,v)\n",
        "  if int(v) > 2:\n",
        "    print(v)\n",
        "    sequences[k] = v'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MJ0_WimvGBBm",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#diff[\"diff in animal area\"] = new[\"diff in animal area\"]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "okscFYt7I1tT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "#diff[\"diff in veg area\"] = new[\"diff in vegetable area\"]"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}